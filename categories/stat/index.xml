<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>stat on Takahiro ONOSHIMA&#39;s website</title>
    <link>https://onoshima.github.io/categories/stat/</link>
    <description>Recent content in stat on Takahiro ONOSHIMA&#39;s website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>Takahiro ONOSHIMA</copyright>
    <lastBuildDate>Sat, 16 Jan 2021 18:38:48 +0900</lastBuildDate><atom:link href="https://onoshima.github.io/categories/stat/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mplusのモンテカルロ法の結果を整形する関数</title>
      <link>https://onoshima.github.io/posts/extract_montecarlo/</link>
      <pubDate>Sat, 16 Jan 2021 18:38:48 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/extract_montecarlo/</guid>
      <description>要約 Mplusのモンテカルロ法の書く繰り返しのパラメータや適合度を整形する関数を書きました。Mplusの.outファイルのパスと各パラメータが保存されたテキストファイルのパスを引数に渡して使います。
extract_montecarlo(filename=&amp;#34;mplus.out&amp;#34;, represult = &amp;#34;represults.txt&amp;#34;) 次のように，ヘッダー付きのデータフレームを出力します。
使いたい人がいるかはわかりませんが，一番最後にコードを置いておきます。
問題 Mplusでモンテカルロ法を用いる場合には，MONTECARLOコマンドのオプションでRESULTS = results_monte5.txt;のように書くと各繰り返しのパラメータの推定値や適合度が記録されます。生成されたファイルは次のような見た目をしています。
それぞれの数字が何を表しているかは，.outファイルの最後の方に書いてあります。
RESULTS SAVING INFORMATION Order of data Replication number Parameter estimates (saved in order shown in Technical 1 output) Standard errors (saved in order shown in Technical 1 output) Chi-square : Value Chi-square : Degrees of Freedom Chi-square : P-Value CFI TLI Number of Free Parameters RMSEA : Estimate SRMR Save file results_monte5.txt Save file format Free 要するに，繰り返しの番号ごとにブロックが区切られていて，パラメータの推定値，パラメータの標準誤差，適合度等という順番に並んでいるのですが，パラメータの順番についてはTechnical 1 outputにある順番で並べてあると書いてあります。Technical outputには推定したパラメータに番号がふられています。先ほどの結果のパラメータはこの番号順に並んでいる訳です。普通のMplusの解析だとOUTPUTコマンドにTECH1を指定すると表示されるようですが，MONTECARLOコマンドを使うと自動で表示されるようです。</description>
    </item>
    
    <item>
      <title>RからMplusを使う</title>
      <link>https://onoshima.github.io/posts/mplusautomation/</link>
      <pubDate>Wed, 13 Jan 2021 15:21:08 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/mplusautomation/</guid>
      <description>構造方程式モデリングのためのソフトウェアと言えばMplusです。Mplusはシンタックスも簡単で計算も早くてとても良いソフトウェアですがアウトプットが見辛い上に，モデルの数だけインプットとアウトプットのファイルが増えていくのでごちゃつきます。また，私だけかもしれませんが，あとで解析をやりなおそうと思った際に，どのファイルを使ったのか分からなくなりがちです。
そこで，MplusをRから使えるパッケージとしてMplusAutomationというものがあります。日本語で解説している資料としては徳岡先生（高松大）によるものあり大変参考になります。
https://www.slideshare.net/masarutokuoka/rmplusmplusautomation-hiroshimar05
以下ではそのスライドを参考にしたり，パッケージのvignetteを見ながら試した記録を残します（主に将来の自分に向けて）。
モデルをRで書く mplusObjectという関数を使うと，Mplusのinputファイルを用意できるようです。以下ではpsychパッケージ所収のbfiデータを使っていきます。とりあえず，5因子のCFAのモデルを書いてみます。
library(psych) library(MplusAutomation) test1 &amp;lt;- mplusObject(MODEL =&amp;#34;F1 BY A1-A5; F2 BY C1-C5; F3 BY E1-E5; F4 BY N1-N5; F5 BY O1-O5&amp;#34;, rdata = bfi) res &amp;lt;- mplusModeler(test, dataout = &amp;#34;test1.tsv&amp;#34;, modelout = &amp;#34;test1.inp&amp;#34;, run = 1L) mplusObject関数の引数は，通常のMplusのsyntaxのセクションに書くような内容を書いていきます。上の例では，モデルの部分だけ書きましたが，書いていないVARIABLEのセクションなどは，MODELの部分から必要なものを選んで勝手に作ってくれているようです。
こうしてできたR上のMplus Model Objectを実行するためのものがmplusModeler関数です。第１の引数には先ほどのMplus Model Objectを指定します。実行すると，Model Objectを元にして，同じディレクトリ内に，Mplusのinput，Mplusの実行用のデータファイル，Mplusのアウトプットが生成されます。dataoutという引数には，Mplus実行用に生成されたデータのファイル名を指定します。このファイルでは，例えば，列名のヘッダーが削除されていたり，欠損値が「.」に置き換えられていたりと，Mplusで処理できる形にデータが変換されています。またmodeloutという引数には，モデルを実行するのに使われたMplusのインプットファイルのファイル名を渡します。runという引数に0を渡すとモデルは実行されず，Mplus用のデータファイルとインプットファイルだけが生成されるようです。モデルが意図した通りに.inpファイルに反映されているかどうかを確認したい時に使うもののようです。1より大きい場合にはブートストラップで繰り返すとのことです。
上の例では，とりあえず実行できれば良いということでモデルのみ書きましたが，もう少しいろいろと指定したものを作ってみます。
test2 &amp;lt;- mplusObject( TITLE = &amp;#34;This is mplusautomation test 2;&amp;#34;, ANALYSIS = &amp;#34;ESTIMATOR = MLR;&amp;#34;, OUTPUT = &amp;#34;SAMPSTAT STDYX MODINDICES(ALL);&amp;#34;, VARIABLE = &amp;#34;USEVARIABLES = A1-A5 C1-C5 E1-E5 N1-N5 O1-O5;&amp;#34;, MODEL =&amp;#34;F1 BY A1-A5*; F2 BY C1-C5*; F3 BY E1-E5*; F4 BY N1-N5*; F5 BY O1-O5*; F1@1 F2@1 F3@1 F4@1 F5@1;&amp;#34;, rdata = bfi) res2 &amp;lt;- mplusModeler(test2, dataout = &amp;#34;test2.</description>
    </item>
    
    <item>
      <title>2段階検定についての統計の教科書の記述</title>
      <link>https://onoshima.github.io/posts/equalvar/</link>
      <pubDate>Tue, 20 Oct 2020 11:22:38 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/equalvar/</guid>
      <description>はじめに 2つの群の平均値の差を検定するときにはt検定がよく使われます。このとき2つの群の分散が異なるときに普通のt検定を行うとタイプIエラーを犯す可能性が大きくなりますが，Welchのt検定を使うとそれをコントロールすることができます。
 https://academic-oup-com.ez.wul.waseda.ac.jp/biomet/article/29/3-4/350/256577  等分散の仮定を違反したときに，どの程度悪いことが起きるのか，そしてWelchの方法を使うとどの程度それがコントロールできるかについては青木先生の次のシミュレーションが大変参考になります。
 http://aoki2.si.gunma-u.ac.jp/lecture/BF/index.html  統計学のテキストの中には，2つの群の等分散の判断にF検定など分散に関する検定を用いて，その結果をもとに普通のt検定を行うかWelchのt検定を用いるかを判断するべし，という記述があったりします。ここではこれを便宜的に2段階検定法と呼びますが，この2段階検定法の問題点は様々なところで指摘されています。井口先生の次のサイトがかなり詳しく書いています。
 https://biolab.sakura.ne.jp/welch-test.html  また，奥村先生の次の簡易的なシミュレーションも参考になります。
 https://oku.edu.mie-u.ac.jp/~okumura/blog/node/2262  ところで，統計解析のテキストはこの問題についてどう書いているのかふと気になりましたので，以下調べてみた記録を以下に残しておきます（手元にあったもののみですが）。
芝・南風原（1990）『行動科学における統計解析法』 2段階検定を推奨している訳ではないですが，そういう手法をとることがあると紹介しています（pp.108-9）。
 ところで，ここで述べた検定法を適用するときは，2群の母集団分散が等しいという仮定が妥当であるかどうかを，実際に得られた標本分散を用いて確認する必要がある。その際，2群の母集団分散に関する仮説$H_0：\sigma_1 ^2 =\sigma_2 ^2$の検定を利用することがある。この仮説が棄却されない場合でも，それが等分散を証明することにはならないが，その仮定が消極的ながら指示されたとみなすのである。
 母分散が等しいという帰無仮説を棄却できなかったからといって，それをもって等分散の証明にならないと明示しているのは親切ですね。
森・吉田（1990）『心理学のためのデータ解析テクニカルブック』 以下の記述（p.62）のように2段階検定を推奨しています。
 2つの条件の平均値の差について検定を行うためには，2条件の分散が等質であるという前提条件が満たされていなければならない。そこで，まず，この条件が満たされているかどうか調べておこう。これにはF検定を行えばよい。
 宮埜（1993）『心理学のためのデータ解析法』 Welchの方法についての紹介はなく普通のt検定のみの解説ですが，等分散性の仮定についての以下のような補足があります。
 実データでは正規性および等分散性の家庭は必ずしも満足されない。しかし，正規性については標本の大きさが15以上ならば満足されなくても検定結果はあまり変わらない。また，等分散性についても分散比が0.4から2.5の範囲にある場合や$n_1 = n_2$である場合にはあまり結果に影響しないことが知られている。
 等分散性についてのこの性質が誰の何の研究に基づいているのかは分からないのですが，巻末にいくつか参考にした文献が挙げられており，そのいずれかにこの記述があるのだと思われます。手元にその文献はないため確認できていませんが。
吉田（1998）『本当にわかりやすいすごく大切なことが書いてあるごく初歩の統計の本』 「対応のない場合のt検定を適用する際の前提条件」というコラム（？）のようなものがあり（p.191）そこには以下のように書いてあります。
 ②両条件の母集団の分散（ないし，標準偏差）が等しいこと。ただし，これも実際には得られたデータの分散が条件間で大きく異らなければよしとします。そして，両条件の分散に顕著な差が認められた場合には，この章の3節で解説するU検定や，ウェルチ（Welch）の検定，コクラン・コックス（Cochran-Cox）の検定などの他の検定を用います。
 コラムの中では分散の異なりの判定については述べていませんが，注には，「2つの条件の分散の差の検定については森・吉田（1990）のF検定の部分を見るように書いているので2段階検定を推奨しているようです。
山田・杉澤・村井（2008）『Rによるやさしい統計学』 t検定に関する章の中で，等分散性の検定を行うように書いています。まとめの段落では以下のように書かれているので，2段階検定が推奨されています（p.150）。
 ここまでの内容を整理しておきましょう。まず，var.test関数を実行して分散の等質性をチェックします。その結果，等分散の仮定が満たされていたら独立な2群のt検定（t.test(クラスA，クラスB，var.equal=TRUE)）を，そうでなかったらWelchの検定（t.test(クラスA, クラスB，var.equal=FALSE)）を行うということです。
 宮川（2015）『基本統計学（第4版）』 Welchの方法についての記載はなく，p.267の注で以下のように書いています。
 $\sigma_1 ^2 \neq \sigma_2 ^2 $ のときの検定方法もあるが，それは本書では扱わない。なお， $\sigma_1 ^2 = \sigma_2 ^2 $としてよいかどうかは，後述（9.7節，275-277ページ）の検定方により調べることができる。
 pp.275-277で紹介されるのはF検定なので，2段階推定を紹介しているようです。</description>
    </item>
    
    <item>
      <title>ggstatsplotパッケージについて</title>
      <link>https://onoshima.github.io/posts/ggstatplot/</link>
      <pubDate>Sun, 18 Oct 2020 10:57:20 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/ggstatplot/</guid>
      <description>ggstatsplotパッケージが出たとのことで少しだけ使ってみた記録です。使い方は以下のリンクより。
https://github.com/IndrajeetPatil/ggstatsplot
ggbetweenstats 群間差の比較のためのバイオリンプロットを出すときに使えます。
library(ggplot2) library(ggstatsplot) p1 &amp;lt;- ggbetweenstats(data=iris, x=Species, y=Sepal.Length) k=3) p1 こんな感じで，検定結果とプロットを一括でやってくれます。便利。 いろいろと細かな設定もできるようです。 p2 &amp;lt;- ggbetweenstats(data=iris, x=Species, y=Sepal.Length, k=2 , #検定結果の小数をどこまで表示するか title=&amp;#34;Distribution of sepal length across Iris species&amp;#34;, plot.type=&amp;#34;box&amp;#34;, # デフォルトはboxviolin，片方だけにもできる pairwise.comparisons = F, #多重比較を表示しないこともできる effsize.type = &amp;#34;biased&amp;#34;, #効果量を偏イータにしたければbiase results.subtitle = F, # FALSEにすると検定結果をサブタイトルから消せる bf.message = F) # したの方にあるベイズファクターの表示の有無 p2 
ggscatterstats 散布図＋ヒストグラムに無相関の検定を合わせて出力するようです。
p3 &amp;lt;- ggscatterstats( data = msleep, x = sleep_rem, y = awake, xlab = &amp;#34;REM sleep (in hours)&amp;#34;, ylab = &amp;#34;Amount of time spent awake (in hours)&amp;#34;, title = &amp;#34;Understanding mammalian sleep&amp;#34;, messages = FALSE ) p3 平均（や中央値）の線を引いたり，特定の値を上回る場合にラベルをつけたりできるみたいです。</description>
    </item>
    
    <item>
      <title>因子分析の教科書における因子数の選択についての記述</title>
      <link>https://onoshima.github.io/posts/factornumber/</link>
      <pubDate>Wed, 23 Sep 2020 14:59:29 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/factornumber/</guid>
      <description>はじめに 必要があって因子分析や心理測定の教科書で探索的因子分析の際の因子数の推定法がどのように扱われているかを調べたものの記録。手元にあるものだけ。
Brown (2006) Brown, T. A. (2006). Confirmatory factor analysis for applied research. The Guilford Press.
因子分析の定評のある教科書（らしい）。大学院の授業で読んだ。基本は確認的因子分析の教科書なのだが，2章で探索的因子分析も扱っている。その中でFactor Selectionという節で（pp.23-30）因子数の推定について扱っている。
よく使われている手法として，Kaiser-Guttmanルール，スクリーテスト，平行分析を紹介している。また，因子の推定法が最尤法のときには，適合度の情報も使えることを紹介している。
Raykov &amp;amp; Marcoulides (2010) Raykov, T., &amp;amp; Marcoulides, G. A. (2010). Introduction to Psychometric Theory. Routledge.
因子分析は第3章で扱われており，因子数の探索は独立した節として3.8節で扱われている。紹介しているのはKaiser基準とスクリープロットとamount of explained varianceを紹介している。文献等の引用はなし。
Tabachnick &amp;amp; Fidell (2012) Tabachnick, B. G., &amp;amp; Fidell, L. S. (2012). Using Multivariate Statistics (6th Edition). Pearson.
因子分析の教科書という訳でなく，多変量統計全般の教科書。13章で主成分分析と因子分析を扱っている。13.6.2のAdequay of Extraction and Number of Factorsという節で因子数についての取り扱いがある（pp648-）。
手法についてのまとめは，Gorsuch (1983)（因子分析の教科書）かZwick and Velicer (1986)を見てねと言い，SPSSとSASで扱える手法に焦点を絞って紹介している。Kairser基準，スクリーテスト，平行分析，MAP基準を紹介している。さらに，Zwick &amp;amp; Velicer (1986)のシミュレーションの結果も紹介して平行分析とMAPがうまくいくと紹介している。</description>
    </item>
    
    <item>
      <title>探索的因子分析の際の平行分析について</title>
      <link>https://onoshima.github.io/posts/parallel/</link>
      <pubDate>Wed, 23 Sep 2020 10:39:35 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/parallel/</guid>
      <description>はじめに 因子分析に関連した調べ物の必要があって，その覚書。今回は平行分析について。
探索的因子分析で因子数を決めるための複数の基準が提案されていますが，平行分析（parallel analysis）はその一つで，因子分析の教科書などでも勧められる方法です。アイデアとしては，分析しようとしているデータセットと同じサイズであるランダムなデータセットから計算された固有値と比べて，ランダムなデータセットよりも大きい固有値までを採用しようという考えのようです。こうすることでサンプリングの誤差に対応できるとのことです。初出はHorn (1965)です。
Rでの実験 Rで平行分析を行うにはpsychパッケージのfa.parallelという関数を用いれば良いのですが，ここでは原理を理解するためにパッケージを使わずにやってみます。使うデータはpsych所収のbfiというデータセットです。ビッグファイブの性格特性の質問紙データで，各因子5項目ずつ計25項目の質問紙のデータです。26列目以降は回答者の属性に関するデータが入っています。サンプルサイズは2800です。
library(psych) dat &amp;lt;- bfi[,1:25] head(dat) A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5 61617 2 4 3 4 4 2 3 3 4 4 3 3 3 4 4 3 4 2 2 3 3 6 3 4 3 61618 2 4 5 2 5 5 4 4 3 4 1 1 6 4 3 3 3 3 5 5 4 2 4 3 3 61620 5 4 5 4 4 4 5 4 2 5 2 4 4 4 5 4 5 4 2 3 4 2 5 5 2 61621 4 4 6 5 5 4 4 3 5 5 5 3 4 4 4 2 5 2 4 1 3 3 4 3 5 61622 2 3 3 4 5 4 4 5 3 2 2 2 5 4 5 2 3 4 4 3 3 3 4 3 3 61623 6 6 5 6 5 6 6 6 1 3 2 1 6 5 6 3 5 2 2 3 4 3 5 6 1 まずは，元データからスクリープロットを描いてみます（15因子目まで）。</description>
    </item>
    
    <item>
      <title>因子分析におけるMAP基準について</title>
      <link>https://onoshima.github.io/posts/vss/</link>
      <pubDate>Tue, 22 Sep 2020 14:41:25 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/vss/</guid>
      <description>はじめに 探索的因子分析において，因子数を決める際にはさまざまな基準があるのですが，その中にMAP基準というものがあります。必要があって調べた際のメモを残しておきます。
MAP基準とは MAPはMinimum average partical correlationの略です。オリジナルはVelicer（1976）による次の論文です。https://doi-org/10.1007/BF02293557
$p$個の観測変数の相関行列を$R$とします。サイズは$p \times p$です。主成分分析の結果である$p \times m$のパターン行列を$A$とします。そのとき，偏分散共分散行列$C^*$は
$$C^* = R- AA^\prime$$
と書けます。ここからさらに偏相関係数の行列は
$$ R^* = D^{-1/2}( R- AA^\prime)D^{-1/2}　\tag{1}　$$
と書けます。ここで $D$は，
$$D = Diag(C^*) = Diag(R- AA^\prime)$$
です。ここで，$R^*$のi行目，j列目の要素を $r ^* _{ij}$としたときに，偏相関行列の2乗の値の平均が
$$ f_m = \frac{\sum \sum_{i \neq j} (r ^* _{ij})^2 }{p(p-1)}　\tag{2}$$
として表されます。MAP基準とは，パターン行列のmの値を変えていきながらこの値を計算して，最小になる$f_m$の$m$を因子数として採択しようという方法です。
実際の数値例を確認 実際にデータを触ってみながら，手順を確認してみます。psychのvss関数の中身を覗きながら書きました。使うデータはpsychパッケージの中に入っているThurstone.9という9つのテストの相関行列です。
library(psych) data(&amp;#34;Thurstone.9&amp;#34;) R &amp;lt;- Thurstone.9     Prefixes Suffixes Vocabulary Sentences First.Last FirstLetters FourLetters Completion SameorOpposite     Prefixes 1.</description>
    </item>
    
    <item>
      <title>EFAから計算したωとCFAから計算したω</title>
      <link>https://onoshima.github.io/posts/omega_efacfa/</link>
      <pubDate>Wed, 16 Sep 2020 15:13:32 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/omega_efacfa/</guid>
      <description>2つのω係数について 信頼性係数の1つであるω係数は$\omega_h$と$\omega_t$があって，それぞれ
$$ \omega_h = \frac{{\bf 1^{\prime} c c^{\prime} 1}}{{\rm Var}(X)} \\ \omega_t = \frac{{\bf 1^{\prime} c c^{\prime} 1}+{\bf 1^{\prime} A A^{\prime} 1}}{{\rm Var}(X)} $$
として表される。${\bf c}$は一般因子の因子負荷量ベクトルで，${\bf A}$は群因子の因子負荷量行列である。
Revelle &amp;amp; Condon（2019）によると，これらの推定値を得る方法は，（1）EFAで高次因子モデルの因子負荷量を得てその結果にScmidt-Leiman変換をかける方法と，（2）CFAで直接的に双因子モデルを指定して推定値を得る方法がある。ここでは，Rで実際にその2つをやってみてどのような違いがあるかを検討する。
使うデータはPsychパッケージに入ってるThurstoneというデータ（相関行列）。9つのテストのデータで，3つの因子（1）Verbal Comprehension,（2）Word Fluency, （3）Reasoningにまとまるようである。サンプルサイズは213らしい。
https://www.personality-project.org/r/html/bifactor.html
探索的因子分析による方法 まずは，探索的因子分析でやってみる。psych所収のomega関数を使う。とても簡単。
library(lavaan) library(psych) library(knitr) data(&amp;#34;Thurstone&amp;#34;) res_efa &amp;lt;- omega(Thurstone) kable(round(res_efa$schmid$sl,3))     g F1* F2* F3* h2 u2 p2     Sentences 0.709 0.560 -0.022 0.030 0.817 0.183 0.615   Vocabulary 0.</description>
    </item>
    
    <item>
      <title>lavaanでMplusのカテゴリカル因子分析を再現する</title>
      <link>https://onoshima.github.io/posts/mplus_lavaan/</link>
      <pubDate>Mon, 27 Apr 2020 13:13:05 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/mplus_lavaan/</guid>
      <description>Mplusで行われたシミュレーションをRで再現しようとした際に調べたことの記録．MplusにおけるESTIMATOR=WLSMVとlavaanでestimator=&amp;quot;WLSMV&amp;quot;にした際の挙動について．
データ Rのpsychのパッケージに入っているbfiの質問紙データの一部（A1-A5の項目まで）を利用した．なぜ一部かというとMplusのデモ版だと使える変数の数に制限があるためである．とりあえずcsvにして書き出す．Mplusだとデータファイルは変数名を含めないようなのでcol_names=FALSEにする．欠損値処理についても考えないために今回はリストワイズ削除．N=2709の5項目（すべて6件法）のデータを使う．
library(readr) library(lavaan) bfiA &amp;lt;- psych::bfi[1:5] bfiA &amp;lt;- na.omit(bfiA) write_csv(bfiA, &amp;#34;bfiA.csv&amp;#34;, col_names = FALSE) head(bfiA) # A1 A2 A3 A4 A5 #61617 2 4 3 4 4 #61618 2 4 5 2 5 #1620 5 4 5 4 4 #61621 4 4 6 5 5 #61622 2 3 3 4 5 #61623 6 6 5 6 5 MplusでCFA まずは，Mplusでやってみる．インプットファイルはこんな感じ．
TITLE: test; DATA: FILE = &amp;quot;bfiA.csv&amp;quot;; VARIABLE: NAMES = A1-A5; USEVARIABLES = A1-A5; CATEGORICAL = A1-A5; ANALYSIS: TYPE = GENERAL; ESTIMATOR= WLSMV; MODEL: F1 BY A1-A5*; F1@1; OUTPUT: SAMPSTAT STDYX MODINDICES(ALL); 結果，モデルフィットの部分は次のような感じ．</description>
    </item>
    
    <item>
      <title>non-normalな乱数を発生するパッケージあれこれ</title>
      <link>https://onoshima.github.io/posts/nonnormal/</link>
      <pubDate>Fri, 24 Apr 2020 10:27:42 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/nonnormal/</guid>
      <description>はじめに Rは基本のパッケージに様々な確率分布の乱数を発生させる関数が備わっているが，研究の都合で正規分布に従わない歪んだデータを発生させる必要がでてきたので，Rで歪んだデータの乱数を発生させる際に使えるパッケージをいくつか試してみた．
fungible まずはfungibleというパッケージに入っているmonte1という関数．変数の数，サンプルサイズ，母相関行列，歪度のベクトル，尖度のベクトルなど必要な情報を入れると，指定した通りの標準化された多変量の歪んだ乱数を生成してくれる．
library(pacman) p_load(fungible) Sigma &amp;lt;- matrix(0.3,nrow=4,ncol=4) diag(Sigma) &amp;lt;- 1 skew &amp;lt;- c(0, 0.25, -0.3, 1.25) kurt &amp;lt;- c(0, 0, 1.5, 2.5) samplesize &amp;lt;- 1000 nvar &amp;lt;- ncol(Sigma) dat &amp;lt;- monte1(seed=1234,nvar=nvar, nsub=samplesize, cormat=Sigma, skewvec=skew, kurtvec=kurt) psych::describe(dat$data) psych::pairs.panels(dat$dat, smooth = F, hist.col = &amp;#34;white&amp;#34;) サンプルサイズが1000程度だと歪度も尖度もそこそこずれているがサンプルサイズを5000程度まで増やすとそれなりに正確になるようだ．
seedの指定が必ず必要なので，シミュレーションとかで使う時はseedをずらしていくようにプログラムを組む必要がある． 乱数発生の手法の元となっているのは，Fleishman(1978)とVale &amp;amp; Maurelli(1983)の論文．
https://link.springer.com/article/10.1007/BF02293811
https://link.springer.com/article/10.1007/BF02293687
SimMultiCorrData 次はSimMultiCorrDataというパッケージに入っているnonnormvar1という関数では1変量の歪んだ乱数を生成することができる．引数には，FleishmanかPolynomialのどちらの方法か，平均，分散，歪度，尖度のほか，サンプルサイズや，seedを指定できる．デフォルトでは平均=0で分散=1である．
p_load(SimMultiCorrData) samplesize &amp;lt;- 1000 dat1 &amp;lt;- nonnormvar1(method=&amp;#34;Fleishman&amp;#34;, skews=-0.6, skurts=0,n=samplesize) dat2 &amp;lt;- nonnormvar1(method=&amp;#34;Fleishman&amp;#34;, skews=0, skurts=1.25,n=samplesize) この関数で発生させた乱数のデータは$continuous_variableに入っており，要約統計量は$summary_continuousにまとまっており，狙い通りの変数ができたかどうかの確認ができて便利である．
ちなみに引数でmethod=&amp;quot;Polynomial&amp;quot;を指定すると次の論文の手順で乱数を生成するようである．</description>
    </item>
    
    <item>
      <title>カテゴリカルデータをlavaanでSEMする際の細かい話</title>
      <link>https://onoshima.github.io/posts/wlsmv/</link>
      <pubDate>Thu, 16 Apr 2020 20:04:15 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/wlsmv/</guid>
      <description>lavaanでカテゴリカルデータのSEMをする際には，orederedの引数にカテゴリカルなデータの変数名を指定する．推定はデフォルトではWLSMVらしい．
WLSMVというのはそもそも，estimatorのみを指している訳でなく，estimatorがDWLSで，robust標準誤差を用いて，平均と分散を調整したテスト統計量を使う3点セットのことらしいのだが，Mplusでデフォルトの推定WLSMVなため業界ではこの呼び名で定着したそうだ．
Mplusはversionが1から5までとversion6以降ではWLSMVの指す意味が違うそうである．lavaanを使ってversion5までのWLSMVを再現するには，estimator=&amp;quot;DWSL&amp;quot;, se=&amp;quot;standard&amp;quot;, test=&amp;quot;Sattertwaite&amp;quot;の3点セットなのだが，ver6以降のWLSMVはestimator=&amp;quot;DWSL&amp;quot;, se=&amp;quot;standard&amp;quot;, test=&amp;quot;scaled.shifted&amp;quot;だそうだ．個別に指定してもよいし，estimator=&amp;quot;WLSMV&amp;quot;とすればver6以降に，estimator=&amp;quot;WLSMVS&amp;quot;とすればver5までのと同じ方法になるようだ．とある推定の結果を一致させたかったときにつまづいたので記録を残しておく．
【参考】
https://personality-project.org/r/tutorials/summerschool.14/rosseel_sem_cat.pdf</description>
    </item>
    
    <item>
      <title>RからeStatのAPIを使ってみる（その2）</title>
      <link>https://onoshima.github.io/posts/restat_packages/</link>
      <pubDate>Sun, 08 Mar 2020 16:50:26 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/restat_packages/</guid>
      <description>はじめに Rからe-Statを使うお勉強の記録．@yutannihilation さん作成のestatapiというパッケージがあり，それを使ったらとても楽だった．使い方は以下のページに書いてある．
https://qiita.com/kazutan/items/9c0b2dd0f055fde45cda
不登校の児童生徒の数 今回は，文部科学省の「児童生徒の問題行動・不登校等生徒指導上の諸課題に関する調査」という調査を使ってみて，都道府県別1000人あたりの不登校児童の数をグラフにしてみた．日本地図については谷村先生作成のNipponMapというパッケージを使った．使い方は以下のサイトを参考にした．
https://oku.edu.mie-u.ac.jp/~okumura/stat/nippon.html
使ったコード library(pacman) p_load(estatapi, keyring,dplyr, stringr,NipponMap,RColorBrewer) key_set(&amp;#34;e-stat&amp;#34;) #　統計表の検索 00400304は児童生徒の問題行動・不登校等生徒指導上の諸課題に関する調査 statsList &amp;lt;- estat_getStatsList( appId = key_get(&amp;#34;e-stat&amp;#34;), searchWord = &amp;#34;&amp;#34;, statsCode = &amp;#34;00400304&amp;#34; ) str(statsList) lists &amp;lt;- statsList %&amp;gt;% filter(str_detect(TITLE, &amp;#34;不登校児童生徒数&amp;#34;)) id_vector &amp;lt;- pull(lists, &amp;#34;@id&amp;#34;) # メタ情報 meta_info &amp;lt;- estat_getMetaInfo(appId = key_get(&amp;#34;e-stat&amp;#34;), statsDataId = id_vector[1]) names(meta_info) # 実際のデータを取得 df &amp;lt;- estat_getStatsData( appId = key_get(&amp;#34;e-stat&amp;#34;), statsDataId = id_vector[1], cdCat02 = c(&amp;#34;120&amp;#34;, &amp;#34;140&amp;#34;), #小学校と中学校 cdArea = meta_info$area$&amp;#34;@code&amp;#34;[2:48]　#1番目は全国なので除外 ) df2017 &amp;lt;- df %&amp;gt;% rename(&amp;#34;year&amp;#34; = &amp;#34;時間軸(年度次)&amp;#34;) %&amp;gt;% filter(year == &amp;#34;2017年度&amp;#34;, 学校種類 == &amp;#34;小学校&amp;#34;, cat01_code == &amp;#34;120&amp;#34;) df2017_jhs &amp;lt;- df %&amp;gt;% rename(&amp;#34;year&amp;#34; = &amp;#34;時間軸(年度次)&amp;#34;) %&amp;gt;% filter(year == &amp;#34;2017年度&amp;#34;, 学校種類 == &amp;#34;中学校&amp;#34;, cat01_code == &amp;#34;120&amp;#34;) # 小学生をplot kenmei &amp;lt;- df2017$地域 br &amp;lt;- seq(3, 9, 1) color &amp;lt;- brewer.</description>
    </item>
    
    <item>
      <title>信頼性係数の目安の出どころ</title>
      <link>https://onoshima.github.io/posts/nunnally/</link>
      <pubDate>Mon, 17 Feb 2020 20:43:28 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/nunnally/</guid>
      <description>尺度が開発される際には信頼性と妥当性についての検討が必要とされます。信頼性の指標としてはクロンバックのαが報告されることが多いのですが，巷でαの値が0.8以上あると望ましいとかなんとか言われていたりします。
例えば次のサイトではこんな風に書かれています。
 α係数の目安は0.80以上です。0.90を超えれば、かなりの信頼性と言えます。
 http://www.u-gakugei.ac.jp/~kishilab/validity-reliability.htm
また別のサイトではこんな風に書かれています。
 通常、アルファ係数が0.8以上であれば一貫性があると見なされます。
 https://bellcurve.jp/statistics/blog/12206.html
ところで，この0.8だの0.9だのの出どころについて調べてみると，英語で書かれたもののいくつかはNunnally(1978)というものを引用しています。これは次の本みたいです。
https://www.amazon.co.jp/Psychometric-Theory-McGraw-Hill-Psychology-Nunnally/dp/0070474656
では，そこには何が書いてあるのかと図書館でみてみますと以下のような記述でした。長いけど引用しておきます。
 What a satisfactory level of reliability is depends on how a measure is being used. In the early stages of research on predictor tests or hypotesized measures of a construct, one saves time and energy by working with instruments that have ony modest reliability, for which purpose reliabilities of .70 or higher will suffice. If significant correlations are found, corrections for attenuation will estimate how much the correlations will increase when reliabilities of measures are increased.</description>
    </item>
    
    <item>
      <title>lavaanのアウトプットをパス図にする</title>
      <link>https://onoshima.github.io/posts/lav2tikz/</link>
      <pubDate>Sat, 08 Feb 2020 11:57:05 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/lav2tikz/</guid>
      <description>はじめに RでSEMをやるときはlavaanをよく使うのですがその結果を綺麗なパス図にしたいということで関数を作りました. lavaanのアウトプットを突っ込むとTikZで描画するためのTeXファイルを吐き出す関数です. 構造方程式を含むもの（フルSEMとか呼ばれるのかな？）には対応しておらず, CFAをそれっぽく出すだけの機能しかありませんが&amp;hellip;
インターネットには有難い情報を提供している方がいるもので, 作る際にはbob3さんの以下のブログが参考になりました.
lavaanで作ったモデルからパス図を描く関数
使い方 みんな大好き Holzinger &amp;amp; Swinefordのデータで試してみます. lavaanのアウトプットを関数に突っ込みます. sizeの引数にはTeXの文字サイズの記号を入れると推定値の文字サイズが変えられます.
library(lavaan) mo &amp;lt;- &amp;#39; visual =~ x1+x2+x3 textual =~ x4+x5+x6 speed =~ x7+x8+x9 &amp;#39; res1 &amp;lt;- cfa(mo, HolzingerSwineford1939) write(lav2tikz(res1, size=&amp;#34;scriptsize&amp;#34;), file=&amp;#34;res1.tex&amp;#34;) 作業ディレクトリにres1.texファイルが作られますので, ターミナルでそこまで移動してコンパイルします.
$ latex res1.tex $ dvipdfmx res1.dvi 次のような結果が得られます（webにアップロード用にpngにしていますが）.
高次因子分析のモデルも一応描画できるようにしています.
mo1 &amp;lt;- &amp;#39; visual =~ x1+x2+x3 textual =~ x4+x5+x6 speed =~ x7+x8+x9 g =~ visual+textual+speed &amp;#39; res2 &amp;lt;- cfa(mo1, data=HolzingerSwineford1939) write(lav2tikz(res2, size=&amp;#34;scriptsize&amp;#34;), file=&amp;#34;res2.tex&amp;#34;) 結果は次の通り.</description>
    </item>
    
    <item>
      <title>続・2群のプリポストデータに関して</title>
      <link>https://onoshima.github.io/posts/prepost2/</link>
      <pubDate>Sat, 01 Feb 2020 20:09:15 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/prepost2/</guid>
      <description>以前に書いたもの 以前に2群のぷりポストデータについての記事を書いた. （2群のプリポストデータに関して）
それに, 関連した文献を読んだの記録を残しておく.
プリポストデータへの分析の使い分け 小杉先生が翻訳したグリム・ヤーノルド『研究論文を読み解くための多変量解析入門応用編』（北大路書房）に書いてある. pp.279-282が該当するページ.
行動科学の研究者は以下の3つの方法を用いる傾向があるそうだ.
 時間を群内変数とし, グループを群間変数とする反復測定のANOVA プレテストを共変量とするANCOVA 差得点に対するANOVA  これらのうちで3つ目を選ぶ人は少ないようである.
最初のANOVAについては「最も倹約的でない方法」とされている. 研究者が興味があるところは, グループ×時間の交互作用で表されており, そこの有意性が確認されたのちに, 単純主効果の検定および多重比較が必要になるので, 複数のステップが必要になる. 交互作用のF値は, 差得点の一元配置のANOVAのF値と等しいので, 後者の方が効率的であるとされている.
差得点のANOVAとANCOVAの使い分けについては研究デザインによるとされている.
具体的には, 実験参加者のグループへの割り当てが完全にランダマインズされたデザインでは, ANCOVAはより強力なツールになるようである
擬似実験だとやや事情が異なり, プレテストのスコアがグループ間で異なる場合には, 差得点の分析を行うのが妥当であり, グループ間でスコアが異ならない場合にはとANCOVAが強力だと書いてある.
上記の説明 詳しくは, Huck &amp;amp; McLean, 1975とMaxwell &amp;amp; Delaney, 1990に書いてあるらしい. （読んでいないけど）
 https://psycnet.apa.org/doiLanding?doi=10.1037%2Fh0076767 https://psycnet.apa.org/record/1990-97136-000  </description>
    </item>
    
    <item>
      <title>ggplot2でpdfを出力する</title>
      <link>https://onoshima.github.io/posts/ggplot2japanese/</link>
      <pubDate>Sun, 12 Jan 2020 23:12:22 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/ggplot2japanese/</guid>
      <description>ggplot2でpdf出力 # ggplot2 pdf出力 library(tidyverse) dat &amp;lt;- as_tibble(iris) dat_long &amp;lt;- dat %&amp;gt;% gather(key=&amp;#34;part&amp;#34;, value=&amp;#34;value&amp;#34;,-Species) %&amp;gt;% mutate(part = as.factor(part)) p &amp;lt;- ggplot(dat_long, aes(x=Species, y=value, fill=Species)) + geom_boxplot() + facet_wrap(~part) # pdfに出力 pdf(&amp;#34;figure/iris.pdf&amp;#34;, width=7, height=7) print(p) dev.off() ※ 画像web表示のためにpng形式.
日本語ファイルを出力 pdfで使うフォントを指定してやるとよい. pdfで何のフォントが使えるかを知りたい時はnames(pdfFonts())とか打つと良い.
# 日本語を使う p1 &amp;lt;- p + ggtitle(&amp;#34;日本語タイトル&amp;#34;) + theme(plot.title=element_text(family=&amp;#34;Japan1GothicBBB&amp;#34;)) pdf(&amp;#34;figure/iris_ja.pdf&amp;#34;, width=7, height=7) print(p1) dev.off() ※ 画像web表示のためにpng形式.
フォントが入っていない環境でも同じように表示するために, 以下のようにしてフォントを埋め込んでおくと良いらしい. embedFonts(&amp;#34;figure/iris.pdf&amp;#34;)</description>
    </item>
    
    <item>
      <title>相関係数行列からクロンバックのαを求める</title>
      <link>https://onoshima.github.io/posts/cronbach/</link>
      <pubDate>Fri, 27 Dec 2019 21:37:22 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/cronbach/</guid>
      <description>尺度の作成論文などでは信頼性係数が報告される。信頼性係数には様々な種類があるが, その中でもっともよく使われるのがクロンバックのαである。クロンバックのαは次の計算式で求められる。
$$ a = \frac{n}{n-1} \left( 1- \frac{\Sigma^n _{i=1}Var(X_i)}{Var(X)}\right) \tag{1} $$
ここで, $n$は項目数, $X_i$は$i$番目の項目の得点である。$X$はテストの合計得点である。
Rでクロンバックのαの求めるには Rでクロンバックのαを求めるためには複数のやり方がある。
http://www.okadajp.org/RWiki/?%CE%B1%E4%BF%82%E6%95%B0%EF%BC%8C%E4%BF%A1%E9%A0%BC%E6%80%A7%E4%BF%82%E6%95%B0%E3%81%AE%E7%AE%97%E5%87%BA
とりあえずここではpsychパッケージに含まれるalpha()関数を使ってみることにする。データは同じパッケージに含まれるbfiを用いる。このデータはビッグファイブ理論が想定する因子を測定する質問紙で1因子につき5項目合計25項目からなる。サンプルサイズは2800である。今回は最初の10項目だけ使う。
データの中身はこんな感じ。6件法である。
さて, このデータに対してクロンバックのαを求めるにはalpha()関数を用いるのだが, データが逆転項目を含んでいるのでそれを処理しなければいけない。6件法なので6から項目を引くと出てくる。ここではのちの話の都合で欠損を含むデータを除外している。
# 逆転項目の処理して必要な項目だけ取り出す new &amp;lt;- dat %&amp;gt;% mutate(A1=6-A1, C4=6-C4, C5=6-C5) %&amp;gt;% select(1:10) %&amp;gt;% filter(!is.na(A1), !is.na(A2), !is.na(A3), !is.na(A4), !is.na(A5), !is.na(C1), !is.na(C2), !is.na(C3), !is.na(C4), !is.na(C5)) 逆転項目の処理が済んだところで, alpha()関数を使ってみる。
# クロンバックαを計算 res &amp;lt;- psych::alpha(new) αの値が出てくるだけでなくて, 各項目をなくした際にαがどのように変化するかの値も出てくる。これらは尺度作成の際には役に立つだろう。
相関係数からクロンバックのαを求める ところで岡田（2015）によれば, クロンバックのαは相関係数行列から求めることもできるようである。
https://www.jstage.jst.go.jp/article/arepj/54/0/54_71/_article/-char/ja/
相関係数行列からクロンバックのαは次の式で求められる。
$$ a = \frac{n\bar{r}}{1+(n-1) \bar{r}} \tag{2} $$
ここでnは項目数で, $\bar{r}$は項目間の相関係数の平均である。さきほどのデータを用いて, 同様の結果が得られるが試してみる。相関係数行列を求めて, 上記の式に当てはめてみる。</description>
    </item>
    
    <item>
      <title>2群のプリポストデータに関して</title>
      <link>https://onoshima.github.io/posts/prepost/</link>
      <pubDate>Sat, 21 Dec 2019 16:12:33 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/prepost/</guid>
      <description>2群のプリポストデータ 理由はよくわからないのだけれど, 知り合いから2群のプリポストデータの分析について聞かれることが多い。何かしらの介入の効果を検討したいらしく、統制群と介入群の2つに分けて, それぞれの群で介入前と介入後のデータを取るみたいな状況を想定しているようだ。
こうした場合に適切な分析手法については吉田（2006）が論じており, （1）ポスト-プリの差得点を取り, その差得点（変化量）を従属変数としてt検定を行う, （2）プリ得点を共変量とした共分散分析（ANCOVA）を行う, の2種類が推奨されている。
2要因（群×時期）の分散分析を行う例が散見されるが, これはこの分析において見たいものが交互作用のみで主効果の分析が意味をなさないので冗長だとされている。なお, この2要因分散分析の結果得られた交互作用のF値は, (1)の手法で得られた検定統計量tの2乗に一致することが知られているそうである。
実際のデータを使った例 実際のデータといっても『やさしく学べる統計学』にあるサンプルデータを使う。データはcsv形式で, 青山学院の寺尾先生のホームページのシラバスからダウンロードできる。
http://www.cc.aoyama.ac.jp/~t41338/lecture/aoyama/stat2e/stat2e_top.html
データの中身の一部を取りだすとこんな感じである。1列目に被験者番号, 2列目に群の情報, 3列目にプリ得点, 4列目にポスト得点。なんの介入なのかは知らないけれどこんな形でデータが得られたとする。統制群と介入群は8名ずつである。
箱ひげ図で表してみると次の通り。
# データセットをlong型にして必要な列をfactorに変換 dat.long&amp;lt;- dat %&amp;gt;% gather(key = time, value = value, -c(group,sub)) %&amp;gt;% mutate(sub = as.factor(sub),group = as.factor(group), time= factor(time, levels=c(&amp;#34;pre&amp;#34;,&amp;#34;post&amp;#34;))) ## データの可視化 ggplot(data=dat.long, aes(x=time,y=value)) + geom_boxplot() + facet_wrap(~group) 差の変数のt検定 まず, （1）の方法を試してみる。 「post-pre」の変数を作ってそれからt検定を行う。
# 差の得点を作ってt検定 newdat &amp;lt;- dat %&amp;gt;% mutate(difference = post-pre) %&amp;gt;% mutate(group = as.factor(group)) res.t &amp;lt;- t.</description>
    </item>
    
    <item>
      <title>テトラコリック相関係数を求める関数の自作</title>
      <link>https://onoshima.github.io/posts/190619tetra/</link>
      <pubDate>Wed, 19 Jun 2019 21:59:21 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/190619tetra/</guid>
      <description>先日テトラコリック相関係数を求める方法についての記事を書いた。
テトラコリック相関の初期
それに基づいてRでテトラコリック相関係数を求める関数を自作した。
Rでの関数化 作った関数は以下の通り。引数には2×2の相関表を指定する。途中のテトラコリック関数（？）で何次まで計算するかということをorder=という引数で指定する。デフォルトは論文に掲載されていたものと同じ6にしてある。
tetrachoric &amp;lt;- function(cor.table, order=6){ #必要な数値を代入 N &amp;lt;- sum(cor.table) prop.bd &amp;lt;- colSums(prop.table(cor.table))[2] prop.cd &amp;lt;- rowSums(prop.table(cor.table))[2] h &amp;lt;- qnorm(prop.bd, lower.tail = F) H &amp;lt;- dnorm(h) k &amp;lt;- qnorm(prop.cd, lower.tail = F) K &amp;lt;- dnorm(k) tau &amp;lt;- numeric(order) theta &amp;lt;- numeric(order) v_bar &amp;lt;- numeric(order) w_bar &amp;lt;- numeric(order) #テトラコリック関数 for(n in 1:order){ if(n==1){ v_bar[n] &amp;lt;- h w_bar[n] &amp;lt;- k tau[n] &amp;lt;- H theta[n] &amp;lt;- K } else if (n==2){ v_bar[n] &amp;lt;- h^2-1 w_bar[n] &amp;lt;- k^2-1 tau[n] &amp;lt;- H*h/sqrt(2) theta[n] &amp;lt;- K*k/sqrt(2) } else { v_bar[n] &amp;lt;- h * v_bar[n-1] - (n-1) * v_bar[n-2] w_bar[n] &amp;lt;- k * w_bar[n-1] - (n-1) * w_bar[n-2] tau[n] &amp;lt;- H*v_bar[n-1]/sqrt(factorial(n)) theta[n] &amp;lt;- K*w_bar[n-1]/sqrt(factorial(n)) } } # 相関を計算 coef_vec &amp;lt;- c(prop.</description>
    </item>
    
    <item>
      <title>テトラコリック相関の初期</title>
      <link>https://onoshima.github.io/posts/190618everitt/</link>
      <pubDate>Tue, 18 Jun 2019 15:10:22 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/190618everitt/</guid>
      <description>テトラコリック相関係数関連の論文を読んだメモ。テトラコリック相関を計算するときに便利な表を作ったぞという話。
書誌情報  Everitt, P. F. (1910). Tables of the tetrachoric functions for fourfold correlation talbes. Biometrika, 7(4), 437–451. https://doi.org/10.1093/biomet/7.4.437
 テーブルの説明 Pearson(1900)が発表したテトラコリック相関では以下のようなクロス表を想定して相関係数を求める。 (p.437)
左の図の実線はx軸とy軸である。点線の表す平面が二変量正規分布を切って4つの区画a,b,c,dに分けられてると考える。周辺確率を用いてhとkの値を求めることができる。
さて，この表から相関係数が求めるには以下の式が使われる。
$$ \frac{d}{N} = \frac{b+d}{N} \cdot \frac{c+d}{N}+S _ {1}^{\infty} (\frac{r^n}{n!}HK\bar{v} _ {n-1}\bar{w} _ {n-1}) \tag{1}$$
ここでHとKは正規曲線の縦座標（確率密度）である（Sは今の$\Sigma$）。$\bar{v}と\bar{w}$ は以下の式で与えられる。
$$ \bar{v} = h\bar{v} _ {n-1} - (n-1)\bar{v} _ {n-2}, \bar{v} _ 0 = 1, \bar{v} _ 1 = h $$ $$ \bar{w} = k\bar{w} _ {n-1} - (n-1)\bar{w} _ {n-2}, \bar{w}_0 = 1, \bar{w} _ 1 = k $$ これらを用いて，</description>
    </item>
    
  </channel>
</rss>
