<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Takahiro ONOSHIMA&#39;s website</title>
    <link>https://onoshima.github.io/</link>
    <description>Recent content on Takahiro ONOSHIMA&#39;s website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>Takahiro ONOSHIMA</copyright>
    <lastBuildDate>Wed, 09 Feb 2022 10:42:17 +0900</lastBuildDate><atom:link href="https://onoshima.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ホームページのデザインの変更</title>
      <link>https://onoshima.github.io/posts/blog-renewal/</link>
      <pubDate>Wed, 09 Feb 2022 10:42:17 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/blog-renewal/</guid>
      <description>ばたばたとしていたのが一段落したので，ホームページのデザインを変更してみた。こういうときにテーマを変えるだけで気分転換ができるのはHugoのいいところだと思う。
資料の整理という観点からは今のテーマは適していないような気がするので，それ用により適したテーマで別のサイトを作ってリンクを貼るのが良さそう。このページはブログ的な使い方に運用を変更する予定。以前は業績も載せていたけどresearchmapと同じ情報を2重に管理するのが面倒なのでresearchmapに一本化することにした。
以下備忘録。contentフォルダの中にstatとかpostsとか下位のフォルダを作って記事を作成していたのだが，今後はpostsに1本化して良いような気がする。stat内の記事をpostsに移すと過去記事のURLが変わってしまうので，外部からリンクが貼られていたことが確認できた記事のみstatフォルダ内に記事は残した。記事が重複してしまうものについては，旧記事のcategoryとtagを消して2重に計上しないようにした。もっとスマートな解決策はあるのだと思うけれどHugoのドキュメントを読む気力はないので，とりあえずこのまま。</description>
    </item>
    
    <item>
      <title>Mplusのモンテカルロ法の結果を整形する関数</title>
      <link>https://onoshima.github.io/posts/extract_montecarlo/</link>
      <pubDate>Sat, 16 Jan 2021 18:38:48 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/extract_montecarlo/</guid>
      <description>要約 Mplusのモンテカルロ法の書く繰り返しのパラメータや適合度を整形する関数を書きました。Mplusの.outファイルのパスと各パラメータが保存されたテキストファイルのパスを引数に渡して使います。
extract_montecarlo(filename=&amp;#34;mplus.out&amp;#34;, represult = &amp;#34;represults.txt&amp;#34;) 次のように，ヘッダー付きのデータフレームを出力します。
使いたい人がいるかはわかりませんが，一番最後にコードを置いておきます。
問題 Mplusでモンテカルロ法を用いる場合には，MONTECARLOコマンドのオプションでRESULTS = results_monte5.txt;のように書くと各繰り返しのパラメータの推定値や適合度が記録されます。生成されたファイルは次のような見た目をしています。
それぞれの数字が何を表しているかは，.outファイルの最後の方に書いてあります。
RESULTS SAVING INFORMATION Order of data Replication number Parameter estimates (saved in order shown in Technical 1 output) Standard errors (saved in order shown in Technical 1 output) Chi-square : Value Chi-square : Degrees of Freedom Chi-square : P-Value CFI TLI Number of Free Parameters RMSEA : Estimate SRMR Save file results_monte5.txt Save file format Free 要するに，繰り返しの番号ごとにブロックが区切られていて，パラメータの推定値，パラメータの標準誤差，適合度等という順番に並んでいるのですが，パラメータの順番についてはTechnical 1 outputにある順番で並べてあると書いてあります。Technical outputには推定したパラメータに番号がふられています。先ほどの結果のパラメータはこの番号順に並んでいる訳です。普通のMplusの解析だとOUTPUTコマンドにTECH1を指定すると表示されるようですが，MONTECARLOコマンドを使うと自動で表示されるようです。</description>
    </item>
    
    <item>
      <title>RからMplusを使う</title>
      <link>https://onoshima.github.io/posts/mplusautomation/</link>
      <pubDate>Wed, 13 Jan 2021 15:21:08 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/mplusautomation/</guid>
      <description>構造方程式モデリングのためのソフトウェアと言えばMplusです。Mplusはシンタックスも簡単で計算も早くてとても良いソフトウェアですがアウトプットが見辛い上に，モデルの数だけインプットとアウトプットのファイルが増えていくのでごちゃつきます。また，私だけかもしれませんが，あとで解析をやりなおそうと思った際に，どのファイルを使ったのか分からなくなりがちです。
そこで，MplusをRから使えるパッケージとしてMplusAutomationというものがあります。日本語で解説している資料としては徳岡先生（高松大）によるものあり大変参考になります。
https://www.slideshare.net/masarutokuoka/rmplusmplusautomation-hiroshimar05
以下ではそのスライドを参考にしたり，パッケージのvignetteを見ながら試した記録を残します（主に将来の自分に向けて）。
モデルをRで書く mplusObjectという関数を使うと，Mplusのinputファイルを用意できるようです。以下ではpsychパッケージ所収のbfiデータを使っていきます。とりあえず，5因子のCFAのモデルを書いてみます。
library(psych) library(MplusAutomation) test1 &amp;lt;- mplusObject(MODEL =&amp;#34;F1 BY A1-A5; F2 BY C1-C5; F3 BY E1-E5; F4 BY N1-N5; F5 BY O1-O5&amp;#34;, rdata = bfi) res &amp;lt;- mplusModeler(test, dataout = &amp;#34;test1.tsv&amp;#34;, modelout = &amp;#34;test1.inp&amp;#34;, run = 1L) mplusObject関数の引数は，通常のMplusのsyntaxのセクションに書くような内容を書いていきます。上の例では，モデルの部分だけ書きましたが，書いていないVARIABLEのセクションなどは，MODELの部分から必要なものを選んで勝手に作ってくれているようです。
こうしてできたR上のMplus Model Objectを実行するためのものがmplusModeler関数です。第１の引数には先ほどのMplus Model Objectを指定します。実行すると，Model Objectを元にして，同じディレクトリ内に，Mplusのinput，Mplusの実行用のデータファイル，Mplusのアウトプットが生成されます。dataoutという引数には，Mplus実行用に生成されたデータのファイル名を指定します。このファイルでは，例えば，列名のヘッダーが削除されていたり，欠損値が「.」に置き換えられていたりと，Mplusで処理できる形にデータが変換されています。またmodeloutという引数には，モデルを実行するのに使われたMplusのインプットファイルのファイル名を渡します。runという引数に0を渡すとモデルは実行されず，Mplus用のデータファイルとインプットファイルだけが生成されるようです。モデルが意図した通りに.inpファイルに反映されているかどうかを確認したい時に使うもののようです。1より大きい場合にはブートストラップで繰り返すとのことです。
上の例では，とりあえず実行できれば良いということでモデルのみ書きましたが，もう少しいろいろと指定したものを作ってみます。
test2 &amp;lt;- mplusObject( TITLE = &amp;#34;This is mplusautomation test 2;&amp;#34;, ANALYSIS = &amp;#34;ESTIMATOR = MLR;&amp;#34;, OUTPUT = &amp;#34;SAMPSTAT STDYX MODINDICES(ALL);&amp;#34;, VARIABLE = &amp;#34;USEVARIABLES = A1-A5 C1-C5 E1-E5 N1-N5 O1-O5;&amp;#34;, MODEL =&amp;#34;F1 BY A1-A5*; F2 BY C1-C5*; F3 BY E1-E5*; F4 BY N1-N5*; F5 BY O1-O5*; F1@1 F2@1 F3@1 F4@1 F5@1;&amp;#34;, rdata = bfi) res2 &amp;lt;- mplusModeler(test2, dataout = &amp;#34;test2.</description>
    </item>
    
    <item>
      <title>tidyverse style guideを読んでみた際のメモ</title>
      <link>https://onoshima.github.io/posts/style/</link>
      <pubDate>Sat, 24 Oct 2020 21:23:55 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/style/</guid>
      <description>tidystyle guideというものがあるのを知ったのでざっと目を通した際の殴り書き。 分析のところまで。
https://style.tidyverse.org/package-files.html#names-1
 ファイル名  空白いれない 意味あるものにする 番号順にしたいなら，1xxx,2xxxxでなくて01xxx,02xxxのように0つめる   内部の構造  # plot data &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-みたいに区切る package読み込むときはファイルの最初でまとめて   変数名  変数名は小文字のみ，意味の区切りは_でつなげる 変数名は名詞で関数名は動詞 cとかTとか一般的な関数や変数に代入しない   空白  ,のあとはスペース入れる ()の前後にスペース入れない ifとかforとかん()の前後はスペース入れる funcutionの()の後にはスペース入れる {{}}の中にはスペース入れる =とか&amp;lt;-とかの前後にはスペース入れる 片側の~式はスペース入れない（複雑だとスペース入れる） =の位置を揃えるための追加のスペースはOK   関数  データの引数名は省略，他は書く   制御フロー  {は行の最後にあるべき 中身は2文字のインデント }は行の最初にあるべき とってもシンプルだったら落として1行でも可 switchでは，各要素1行，フォールスルーエラーを書く   長い行  80文字を超えない 関数名が長いなら，改行して1行ずつ，引数の指定する。 引数を意味の繋がりで行にまとめても良い   セミコロン  行の終わりで使わない。1行に2つのコマンドを詰め込まない 変数の代入に=を使わない&amp;lt;-を使う   データ  文字列ベクトルは&amp;quot;&amp;ldquo;にする。&amp;lsquo;&amp;lsquo;は事情がない限り使わない。 論理値ベクトルはT，FよりもTRUE，FALSE推奨   コメント  #とスペースで始める コメントをつける必要がある場合にはまずコードが明確にならないかを検討する コメントの方が長いならRmarkdownを使う   関数  return()は関数内の早い段階の場合のみ使う。最後の式が評価されるのだから return使うときには独立した１つの行に   コメント（関数） - 何をやっているかでなく，なぜやっているかを書く - 文の形で書く。2文以上あるときだけピリオド パイプ  %&amp;gt;%使うと流れがわかりやすい %&amp;gt;%の前にはスペース，改行して使う。空白は2文字。 magrittrは()を省略できるけどするな   ggplot  ggplotの+は基本%&amp;gt;%と同じように使う dplyrの%&amp;gt;%と合わせて使うなら，さらに深いインデントにしない（インデントのレベルは1まで） ggplot関数の内部でデータの操作もできるけどしない。ggplotに渡す前にデータの操作をしておく    </description>
    </item>
    
    <item>
      <title>2段階検定についての統計の教科書の記述</title>
      <link>https://onoshima.github.io/posts/equalvar/</link>
      <pubDate>Tue, 20 Oct 2020 11:22:38 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/equalvar/</guid>
      <description>はじめに 2つの群の平均値の差を検定するときにはt検定がよく使われます。このとき2つの群の分散が異なるときに普通のt検定を行うとタイプIエラーを犯す可能性が大きくなりますが，Welchのt検定を使うとそれをコントロールすることができます。
 https://academic-oup-com.ez.wul.waseda.ac.jp/biomet/article/29/3-4/350/256577  等分散の仮定を違反したときに，どの程度悪いことが起きるのか，そしてWelchの方法を使うとどの程度それがコントロールできるかについては青木先生の次のシミュレーションが大変参考になります。
 http://aoki2.si.gunma-u.ac.jp/lecture/BF/index.html  統計学のテキストの中には，2つの群の等分散の判断にF検定など分散に関する検定を用いて，その結果をもとに普通のt検定を行うかWelchのt検定を用いるかを判断するべし，という記述があったりします。ここではこれを便宜的に2段階検定法と呼びますが，この2段階検定法の問題点は様々なところで指摘されています。井口先生の次のサイトがかなり詳しく書いています。
 https://biolab.sakura.ne.jp/welch-test.html  また，奥村先生の次の簡易的なシミュレーションも参考になります。
 https://oku.edu.mie-u.ac.jp/~okumura/blog/node/2262  ところで，統計解析のテキストはこの問題についてどう書いているのかふと気になりましたので，以下調べてみた記録を以下に残しておきます（手元にあったもののみですが）。
芝・南風原（1990）『行動科学における統計解析法』 2段階検定を推奨している訳ではないですが，そういう手法をとることがあると紹介しています（pp.108-9）。
 ところで，ここで述べた検定法を適用するときは，2群の母集団分散が等しいという仮定が妥当であるかどうかを，実際に得られた標本分散を用いて確認する必要がある。その際，2群の母集団分散に関する仮説$H_0：\sigma_1 ^2 =\sigma_2 ^2$の検定を利用することがある。この仮説が棄却されない場合でも，それが等分散を証明することにはならないが，その仮定が消極的ながら指示されたとみなすのである。
 母分散が等しいという帰無仮説を棄却できなかったからといって，それをもって等分散の証明にならないと明示しているのは親切ですね。
森・吉田（1990）『心理学のためのデータ解析テクニカルブック』 以下の記述（p.62）のように2段階検定を推奨しています。
 2つの条件の平均値の差について検定を行うためには，2条件の分散が等質であるという前提条件が満たされていなければならない。そこで，まず，この条件が満たされているかどうか調べておこう。これにはF検定を行えばよい。
 宮埜（1993）『心理学のためのデータ解析法』 Welchの方法についての紹介はなく普通のt検定のみの解説ですが，等分散性の仮定についての以下のような補足があります。
 実データでは正規性および等分散性の家庭は必ずしも満足されない。しかし，正規性については標本の大きさが15以上ならば満足されなくても検定結果はあまり変わらない。また，等分散性についても分散比が0.4から2.5の範囲にある場合や$n_1 = n_2$である場合にはあまり結果に影響しないことが知られている。
 等分散性についてのこの性質が誰の何の研究に基づいているのかは分からないのですが，巻末にいくつか参考にした文献が挙げられており，そのいずれかにこの記述があるのだと思われます。手元にその文献はないため確認できていませんが。
吉田（1998）『本当にわかりやすいすごく大切なことが書いてあるごく初歩の統計の本』 「対応のない場合のt検定を適用する際の前提条件」というコラム（？）のようなものがあり（p.191）そこには以下のように書いてあります。
 ②両条件の母集団の分散（ないし，標準偏差）が等しいこと。ただし，これも実際には得られたデータの分散が条件間で大きく異らなければよしとします。そして，両条件の分散に顕著な差が認められた場合には，この章の3節で解説するU検定や，ウェルチ（Welch）の検定，コクラン・コックス（Cochran-Cox）の検定などの他の検定を用います。
 コラムの中では分散の異なりの判定については述べていませんが，注には，「2つの条件の分散の差の検定については森・吉田（1990）のF検定の部分を見るように書いているので2段階検定を推奨しているようです。
山田・杉澤・村井（2008）『Rによるやさしい統計学』 t検定に関する章の中で，等分散性の検定を行うように書いています。まとめの段落では以下のように書かれているので，2段階検定が推奨されています（p.150）。
 ここまでの内容を整理しておきましょう。まず，var.test関数を実行して分散の等質性をチェックします。その結果，等分散の仮定が満たされていたら独立な2群のt検定（t.test(クラスA，クラスB，var.equal=TRUE)）を，そうでなかったらWelchの検定（t.test(クラスA, クラスB，var.equal=FALSE)）を行うということです。
 宮川（2015）『基本統計学（第4版）』 Welchの方法についての記載はなく，p.267の注で以下のように書いています。
 $\sigma_1 ^2 \neq \sigma_2 ^2 $ のときの検定方法もあるが，それは本書では扱わない。なお， $\sigma_1 ^2 = \sigma_2 ^2 $としてよいかどうかは，後述（9.7節，275-277ページ）の検定方により調べることができる。
 pp.275-277で紹介されるのはF検定なので，2段階推定を紹介しているようです。</description>
    </item>
    
    <item>
      <title>ggstatsplotパッケージについて</title>
      <link>https://onoshima.github.io/posts/ggstatplot/</link>
      <pubDate>Sun, 18 Oct 2020 10:57:20 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/ggstatplot/</guid>
      <description>ggstatsplotパッケージが出たとのことで少しだけ使ってみた記録です。使い方は以下のリンクより。
https://github.com/IndrajeetPatil/ggstatsplot
ggbetweenstats 群間差の比較のためのバイオリンプロットを出すときに使えます。
library(ggplot2) library(ggstatsplot) p1 &amp;lt;- ggbetweenstats(data=iris, x=Species, y=Sepal.Length) k=3) p1 こんな感じで，検定結果とプロットを一括でやってくれます。便利。 いろいろと細かな設定もできるようです。 p2 &amp;lt;- ggbetweenstats(data=iris, x=Species, y=Sepal.Length, k=2 , #検定結果の小数をどこまで表示するか title=&amp;#34;Distribution of sepal length across Iris species&amp;#34;, plot.type=&amp;#34;box&amp;#34;, # デフォルトはboxviolin，片方だけにもできる pairwise.comparisons = F, #多重比較を表示しないこともできる effsize.type = &amp;#34;biased&amp;#34;, #効果量を偏イータにしたければbiase results.subtitle = F, # FALSEにすると検定結果をサブタイトルから消せる bf.message = F) # したの方にあるベイズファクターの表示の有無 p2 
ggscatterstats 散布図＋ヒストグラムに無相関の検定を合わせて出力するようです。
p3 &amp;lt;- ggscatterstats( data = msleep, x = sleep_rem, y = awake, xlab = &amp;#34;REM sleep (in hours)&amp;#34;, ylab = &amp;#34;Amount of time spent awake (in hours)&amp;#34;, title = &amp;#34;Understanding mammalian sleep&amp;#34;, messages = FALSE ) p3 平均（や中央値）の線を引いたり，特定の値を上回る場合にラベルをつけたりできるみたいです。</description>
    </item>
    
    <item>
      <title>gtパッケージについて</title>
      <link>https://onoshima.github.io/posts/gt/</link>
      <pubDate>Thu, 08 Oct 2020 18:31:14 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/gt/</guid>
      <description>Rにgtというパッケージがあります。これは，お手軽に表を作るためのパッケージです。使い方については以下の記事を見ると良いです。
 https://qiita.com/yanami/items/8684640b20c86594fbec https://qiita.com/yanami/items/117851de49024f5980d0  ここでは私の自分のための備忘録を残しています。
データの整形 irisデータで実験してみる。表の表示に適するように前段階としてデータを整形する。
library(gt) library(tidyverse) dat &amp;lt;- tibble(iris) %&amp;gt;% group_by(Species) %&amp;gt;% summarise(across(Sepal.Length:Petal.Width,mean)) %&amp;gt;% pivot_longer(col=-Species, names_to=c(&amp;#34;S/P&amp;#34;,&amp;#34;L/W&amp;#34;), names_sep=&amp;#34;\\.&amp;#34;) %&amp;gt;% pivot_wider(names_from = &amp;#34;L/W&amp;#34;) gt関数 とりあえずこのままで，gt関数に突っ込んでみるとそれらしい表を出力してみす。
dat %&amp;gt;% gt() グループに分けて表示するには，groupname_colの引数にグループ分けしたい列名を入れます。
ab1 &amp;lt;- gt(dat, groupname_col = &amp;#34;Species&amp;#34;, rowname_col = &amp;#34;S/P&amp;#34;) tab1 表示する桁数が多いので減らしてみましょう。先ほど作ったものに桁数の設定を行う関数をたします。ここら辺の感覚はggplot的です。
tab2 &amp;lt;- tab1 %&amp;gt;% fmt_number( columns = vars(&amp;#34;Length&amp;#34;, &amp;#34;Width&amp;#34;), decimals = 2) tab2 今度はタイトルを足してみます。タイトルを設定する関数をパイプでつなぎます。ついでスタブ列の列名も足すことにします。
tab3 &amp;lt;- tab2 %&amp;gt;% tab_header( title = &amp;#34;ここにタイトルが入るよ&amp;#34;, ) %&amp;gt;% tab_stubhead(&amp;#34;S/P&amp;#34;) %&amp;gt;% tab_style( style=cell_text(align=&amp;#34;left&amp;#34;), locations = cells_title(&amp;#34;title&amp;#34;)) tab3  ついでに注もつけてみます。</description>
    </item>
    
    <item>
      <title>Anacondaによる環境設定</title>
      <link>https://onoshima.github.io/posts/anaconda/</link>
      <pubDate>Thu, 24 Sep 2020 09:30:10 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/anaconda/</guid>
      <description>必要があってPythonをそこそこ久しぶりにいじることになったので作業内容のメモを残して置くことにします。 今回はAnacondaを用いた環境構築のやり方です。
Anacondaとは Anacondaはデータサイエンスや科学技術に必要なツールやライブラリをまとめてインストールできるものです。面倒な環境構築を避けることができるというのが魅力のようです。PythonとAnacondaの違いについては以下のサイトに詳しく載っています。 https://www.python.jp/install/docs/pypi_or_anaconda.html
Anacondaのインストール 次のサイトに指示に従ってやっていきます。 https://www.python.jp/install/anaconda/index.html
まずは，Anacondaの公式ホームページからインストーラーをダウンロードします。
下の方にスクロールしていくとOSごとにインストーラーがあるので必要なものをダウンロードします。今回はグラフィカルインストーラーを選びます。
ダウンロードしたファイルを開くとインストーラが起動します。指示に従って進めていきます。インストール先については「自分専用にインストール」を選んでいます。
無事にインストールされましたので起動してみます。
コマンドライン環境の設定 ターミナルからAnacondaのPythonを有効にするためには，
/opt/anaconda3/bin/conda init zsh とするようです。 https://www.python.jp/install/anaconda/macos/install.html
このコマンドの後，ターミナルを再起動すると，conda環境である（base）というのが表示されます。 （ただ，私がやったときにはこのコマンド打たなくてもなぜか勝手にコンダ環境が自動設定されていました。私の勘違いかもしれませんが。）
コンダ環境でpythonとコマンドを打つと，AnacondaのPythonが起動することが分かります。
(base) ~ ❯❯❯ python Python 3.8.3 (default, Jul 2 2020, 11:26:31) [Clang 10.0.0 ] :: Anaconda, Inc. on darwin Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information. &amp;gt;&amp;gt;&amp;gt; コンダ環境から抜けたいときはconda deactivateと打てばOKです。
とりあえずここまで。</description>
    </item>
    
    <item>
      <title>因子分析の教科書における因子数の選択についての記述</title>
      <link>https://onoshima.github.io/posts/factornumber/</link>
      <pubDate>Wed, 23 Sep 2020 14:59:29 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/factornumber/</guid>
      <description>はじめに 必要があって因子分析や心理測定の教科書で探索的因子分析の際の因子数の推定法がどのように扱われているかを調べたものの記録。手元にあるものだけ。
Brown (2006) Brown, T. A. (2006). Confirmatory factor analysis for applied research. The Guilford Press.
因子分析の定評のある教科書（らしい）。大学院の授業で読んだ。基本は確認的因子分析の教科書なのだが，2章で探索的因子分析も扱っている。その中でFactor Selectionという節で（pp.23-30）因子数の推定について扱っている。
よく使われている手法として，Kaiser-Guttmanルール，スクリーテスト，平行分析を紹介している。また，因子の推定法が最尤法のときには，適合度の情報も使えることを紹介している。
Raykov &amp;amp; Marcoulides (2010) Raykov, T., &amp;amp; Marcoulides, G. A. (2010). Introduction to Psychometric Theory. Routledge.
因子分析は第3章で扱われており，因子数の探索は独立した節として3.8節で扱われている。紹介しているのはKaiser基準とスクリープロットとamount of explained varianceを紹介している。文献等の引用はなし。
Tabachnick &amp;amp; Fidell (2012) Tabachnick, B. G., &amp;amp; Fidell, L. S. (2012). Using Multivariate Statistics (6th Edition). Pearson.
因子分析の教科書という訳でなく，多変量統計全般の教科書。13章で主成分分析と因子分析を扱っている。13.6.2のAdequay of Extraction and Number of Factorsという節で因子数についての取り扱いがある（pp648-）。
手法についてのまとめは，Gorsuch (1983)（因子分析の教科書）かZwick and Velicer (1986)を見てねと言い，SPSSとSASで扱える手法に焦点を絞って紹介している。Kairser基準，スクリーテスト，平行分析，MAP基準を紹介している。さらに，Zwick &amp;amp; Velicer (1986)のシミュレーションの結果も紹介して平行分析とMAPがうまくいくと紹介している。</description>
    </item>
    
    <item>
      <title>探索的因子分析の際の平行分析について</title>
      <link>https://onoshima.github.io/posts/parallel/</link>
      <pubDate>Wed, 23 Sep 2020 10:39:35 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/parallel/</guid>
      <description>はじめに 因子分析に関連した調べ物の必要があって，その覚書。今回は平行分析について。
探索的因子分析で因子数を決めるための複数の基準が提案されていますが，平行分析（parallel analysis）はその一つで，因子分析の教科書などでも勧められる方法です。アイデアとしては，分析しようとしているデータセットと同じサイズであるランダムなデータセットから計算された固有値と比べて，ランダムなデータセットよりも大きい固有値までを採用しようという考えのようです。こうすることでサンプリングの誤差に対応できるとのことです。初出はHorn (1965)です。
Rでの実験 Rで平行分析を行うにはpsychパッケージのfa.parallelという関数を用いれば良いのですが，ここでは原理を理解するためにパッケージを使わずにやってみます。使うデータはpsych所収のbfiというデータセットです。ビッグファイブの性格特性の質問紙データで，各因子5項目ずつ計25項目の質問紙のデータです。26列目以降は回答者の属性に関するデータが入っています。サンプルサイズは2800です。
library(psych) dat &amp;lt;- bfi[,1:25] head(dat) A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5 61617 2 4 3 4 4 2 3 3 4 4 3 3 3 4 4 3 4 2 2 3 3 6 3 4 3 61618 2 4 5 2 5 5 4 4 3 4 1 1 6 4 3 3 3 3 5 5 4 2 4 3 3 61620 5 4 5 4 4 4 5 4 2 5 2 4 4 4 5 4 5 4 2 3 4 2 5 5 2 61621 4 4 6 5 5 4 4 3 5 5 5 3 4 4 4 2 5 2 4 1 3 3 4 3 5 61622 2 3 3 4 5 4 4 5 3 2 2 2 5 4 5 2 3 4 4 3 3 3 4 3 3 61623 6 6 5 6 5 6 6 6 1 3 2 1 6 5 6 3 5 2 2 3 4 3 5 6 1 まずは，元データからスクリープロットを描いてみます（15因子目まで）。</description>
    </item>
    
    <item>
      <title>因子分析におけるMAP基準について</title>
      <link>https://onoshima.github.io/posts/vss/</link>
      <pubDate>Tue, 22 Sep 2020 14:41:25 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/vss/</guid>
      <description>はじめに 探索的因子分析において，因子数を決める際にはさまざまな基準があるのですが，その中にMAP基準というものがあります。必要があって調べた際のメモを残しておきます。
MAP基準とは MAPはMinimum average partical correlationの略です。オリジナルはVelicer（1976）による次の論文です。https://doi-org/10.1007/BF02293557
$p$個の観測変数の相関行列を$R$とします。サイズは$p \times p$です。主成分分析の結果である$p \times m$のパターン行列を$A$とします。そのとき，偏分散共分散行列$C^*$は
$$C^* = R- AA^\prime$$
と書けます。ここからさらに偏相関係数の行列は
$$ R^* = D^{-1/2}( R- AA^\prime)D^{-1/2}　\tag{1}　$$
と書けます。ここで $D$は，
$$D = Diag(C^*) = Diag(R- AA^\prime)$$
です。ここで，$R^*$のi行目，j列目の要素を $r ^* _{ij}$としたときに，偏相関行列の2乗の値の平均が
$$ f_m = \frac{\sum \sum_{i \neq j} (r ^* _{ij})^2 }{p(p-1)}　\tag{2}$$
として表されます。MAP基準とは，パターン行列のmの値を変えていきながらこの値を計算して，最小になる$f_m$の$m$を因子数として採択しようという方法です。
実際の数値例を確認 実際にデータを触ってみながら，手順を確認してみます。psychのvss関数の中身を覗きながら書きました。使うデータはpsychパッケージの中に入っているThurstone.9という9つのテストの相関行列です。
library(psych) data(&amp;#34;Thurstone.9&amp;#34;) R &amp;lt;- Thurstone.9     Prefixes Suffixes Vocabulary Sentences First.Last FirstLetters FourLetters Completion SameorOpposite     Prefixes 1.</description>
    </item>
    
    <item>
      <title>EFAから計算したωとCFAから計算したω</title>
      <link>https://onoshima.github.io/posts/omega_efacfa/</link>
      <pubDate>Wed, 16 Sep 2020 15:13:32 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/omega_efacfa/</guid>
      <description>2つのω係数について 信頼性係数の1つであるω係数は$\omega_h$と$\omega_t$があって，それぞれ
$$ \omega_h = \frac{{\bf 1^{\prime} c c^{\prime} 1}}{{\rm Var}(X)} \\ \omega_t = \frac{{\bf 1^{\prime} c c^{\prime} 1}+{\bf 1^{\prime} A A^{\prime} 1}}{{\rm Var}(X)} $$
として表される。${\bf c}$は一般因子の因子負荷量ベクトルで，${\bf A}$は群因子の因子負荷量行列である。
Revelle &amp;amp; Condon（2019）によると，これらの推定値を得る方法は，（1）EFAで高次因子モデルの因子負荷量を得てその結果にScmidt-Leiman変換をかける方法と，（2）CFAで直接的に双因子モデルを指定して推定値を得る方法がある。ここでは，Rで実際にその2つをやってみてどのような違いがあるかを検討する。
使うデータはPsychパッケージに入ってるThurstoneというデータ（相関行列）。9つのテストのデータで，3つの因子（1）Verbal Comprehension,（2）Word Fluency, （3）Reasoningにまとまるようである。サンプルサイズは213らしい。
https://www.personality-project.org/r/html/bifactor.html
探索的因子分析による方法 まずは，探索的因子分析でやってみる。psych所収のomega関数を使う。とても簡単。
library(lavaan) library(psych) library(knitr) data(&amp;#34;Thurstone&amp;#34;) res_efa &amp;lt;- omega(Thurstone) kable(round(res_efa$schmid$sl,3))     g F1* F2* F3* h2 u2 p2     Sentences 0.709 0.560 -0.022 0.030 0.817 0.183 0.615   Vocabulary 0.</description>
    </item>
    
    <item>
      <title>lavaanでMplusのカテゴリカル因子分析を再現する</title>
      <link>https://onoshima.github.io/posts/mplus_lavaan/</link>
      <pubDate>Mon, 27 Apr 2020 13:13:05 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/mplus_lavaan/</guid>
      <description>Mplusで行われたシミュレーションをRで再現しようとした際に調べたことの記録．MplusにおけるESTIMATOR=WLSMVとlavaanでestimator=&amp;quot;WLSMV&amp;quot;にした際の挙動について．
データ Rのpsychのパッケージに入っているbfiの質問紙データの一部（A1-A5の項目まで）を利用した．なぜ一部かというとMplusのデモ版だと使える変数の数に制限があるためである．とりあえずcsvにして書き出す．Mplusだとデータファイルは変数名を含めないようなのでcol_names=FALSEにする．欠損値処理についても考えないために今回はリストワイズ削除．N=2709の5項目（すべて6件法）のデータを使う．
library(readr) library(lavaan) bfiA &amp;lt;- psych::bfi[1:5] bfiA &amp;lt;- na.omit(bfiA) write_csv(bfiA, &amp;#34;bfiA.csv&amp;#34;, col_names = FALSE) head(bfiA) # A1 A2 A3 A4 A5 #61617 2 4 3 4 4 #61618 2 4 5 2 5 #1620 5 4 5 4 4 #61621 4 4 6 5 5 #61622 2 3 3 4 5 #61623 6 6 5 6 5 MplusでCFA まずは，Mplusでやってみる．インプットファイルはこんな感じ．
TITLE: test; DATA: FILE = &amp;quot;bfiA.csv&amp;quot;; VARIABLE: NAMES = A1-A5; USEVARIABLES = A1-A5; CATEGORICAL = A1-A5; ANALYSIS: TYPE = GENERAL; ESTIMATOR= WLSMV; MODEL: F1 BY A1-A5*; F1@1; OUTPUT: SAMPSTAT STDYX MODINDICES(ALL); 結果，モデルフィットの部分は次のような感じ．</description>
    </item>
    
    <item>
      <title>non-normalな乱数を発生するパッケージあれこれ</title>
      <link>https://onoshima.github.io/posts/nonnormal/</link>
      <pubDate>Fri, 24 Apr 2020 10:27:42 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/nonnormal/</guid>
      <description>はじめに Rは基本のパッケージに様々な確率分布の乱数を発生させる関数が備わっているが，研究の都合で正規分布に従わない歪んだデータを発生させる必要がでてきたので，Rで歪んだデータの乱数を発生させる際に使えるパッケージをいくつか試してみた．
fungible まずはfungibleというパッケージに入っているmonte1という関数．変数の数，サンプルサイズ，母相関行列，歪度のベクトル，尖度のベクトルなど必要な情報を入れると，指定した通りの標準化された多変量の歪んだ乱数を生成してくれる．
library(pacman) p_load(fungible) Sigma &amp;lt;- matrix(0.3,nrow=4,ncol=4) diag(Sigma) &amp;lt;- 1 skew &amp;lt;- c(0, 0.25, -0.3, 1.25) kurt &amp;lt;- c(0, 0, 1.5, 2.5) samplesize &amp;lt;- 1000 nvar &amp;lt;- ncol(Sigma) dat &amp;lt;- monte1(seed=1234,nvar=nvar, nsub=samplesize, cormat=Sigma, skewvec=skew, kurtvec=kurt) psych::describe(dat$data) psych::pairs.panels(dat$dat, smooth = F, hist.col = &amp;#34;white&amp;#34;) サンプルサイズが1000程度だと歪度も尖度もそこそこずれているがサンプルサイズを5000程度まで増やすとそれなりに正確になるようだ．
seedの指定が必ず必要なので，シミュレーションとかで使う時はseedをずらしていくようにプログラムを組む必要がある． 乱数発生の手法の元となっているのは，Fleishman(1978)とVale &amp;amp; Maurelli(1983)の論文．
https://link.springer.com/article/10.1007/BF02293811
https://link.springer.com/article/10.1007/BF02293687
SimMultiCorrData 次はSimMultiCorrDataというパッケージに入っているnonnormvar1という関数では1変量の歪んだ乱数を生成することができる．引数には，FleishmanかPolynomialのどちらの方法か，平均，分散，歪度，尖度のほか，サンプルサイズや，seedを指定できる．デフォルトでは平均=0で分散=1である．
p_load(SimMultiCorrData) samplesize &amp;lt;- 1000 dat1 &amp;lt;- nonnormvar1(method=&amp;#34;Fleishman&amp;#34;, skews=-0.6, skurts=0,n=samplesize) dat2 &amp;lt;- nonnormvar1(method=&amp;#34;Fleishman&amp;#34;, skews=0, skurts=1.25,n=samplesize) この関数で発生させた乱数のデータは$continuous_variableに入っており，要約統計量は$summary_continuousにまとまっており，狙い通りの変数ができたかどうかの確認ができて便利である．
ちなみに引数でmethod=&amp;quot;Polynomial&amp;quot;を指定すると次の論文の手順で乱数を生成するようである．</description>
    </item>
    
    <item>
      <title>カテゴリカルデータをlavaanでSEMする際の細かい話</title>
      <link>https://onoshima.github.io/posts/wlsmv/</link>
      <pubDate>Thu, 16 Apr 2020 20:04:15 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/wlsmv/</guid>
      <description>lavaanでカテゴリカルデータのSEMをする際には，orederedの引数にカテゴリカルなデータの変数名を指定する．推定はデフォルトではWLSMVらしい．
WLSMVというのはそもそも，estimatorのみを指している訳でなく，estimatorがDWLSで，robust標準誤差を用いて，平均と分散を調整したテスト統計量を使う3点セットのことらしいのだが，Mplusでデフォルトの推定WLSMVなため業界ではこの呼び名で定着したそうだ．
Mplusはversionが1から5までとversion6以降ではWLSMVの指す意味が違うそうである．lavaanを使ってversion5までのWLSMVを再現するには，estimator=&amp;quot;DWSL&amp;quot;, se=&amp;quot;standard&amp;quot;, test=&amp;quot;Sattertwaite&amp;quot;の3点セットなのだが，ver6以降のWLSMVはestimator=&amp;quot;DWSL&amp;quot;, se=&amp;quot;standard&amp;quot;, test=&amp;quot;scaled.shifted&amp;quot;だそうだ．個別に指定してもよいし，estimator=&amp;quot;WLSMV&amp;quot;とすればver6以降に，estimator=&amp;quot;WLSMVS&amp;quot;とすればver5までのと同じ方法になるようだ．とある推定の結果を一致させたかったときにつまづいたので記録を残しておく．
【参考】
https://personality-project.org/r/tutorials/summerschool.14/rosseel_sem_cat.pdf</description>
    </item>
    
    <item>
      <title>カテゴリカルデータをlavaanでSEMする際の細かい話</title>
      <link>https://onoshima.github.io/stat/wlsmv/</link>
      <pubDate>Thu, 16 Apr 2020 20:04:15 +0900</pubDate>
      
      <guid>https://onoshima.github.io/stat/wlsmv/</guid>
      <description>lavaanでカテゴリカルデータのSEMをする際には，orederedの引数にカテゴリカルなデータの変数名を指定する．推定はデフォルトではWLSMVらしい．
WLSMVというのはそもそも，estimatorのみを指している訳でなく，estimatorがDWLSで，robust標準誤差を用いて，平均と分散を調整したテスト統計量を使う3点セットのことらしいのだが，Mplusでデフォルトの推定WLSMVなため業界ではこの呼び名で定着したそうだ．
Mplusはversionが1から5までとversion6以降ではWLSMVの指す意味が違うそうである．lavaanを使ってversion5までのWLSMVを再現するには，estimator=&amp;quot;DWSL&amp;quot;, se=&amp;quot;standard&amp;quot;, test=&amp;quot;Sattertwaite&amp;quot;の3点セットなのだが，ver6以降のWLSMVはestimator=&amp;quot;DWSL&amp;quot;, se=&amp;quot;standard&amp;quot;, test=&amp;quot;scaled.shifted&amp;quot;だそうだ．個別に指定してもよいし，estimator=&amp;quot;WLSMV&amp;quot;とすればver6以降に，estimator=&amp;quot;WLSMVS&amp;quot;とすればver5までのと同じ方法になるようだ．とある推定の結果を一致させたかったときにつまづいたので記録を残しておく．
【参考】
https://personality-project.org/r/tutorials/summerschool.14/rosseel_sem_cat.pdf</description>
    </item>
    
    <item>
      <title>Zoteroのファイル管理の覚書</title>
      <link>https://onoshima.github.io/posts/zotero/</link>
      <pubDate>Thu, 09 Apr 2020 20:54:52 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/zotero/</guid>
      <description>はじめに 文献管理にZoteroを使っています．ZoteroのWeb importerを使って論文をダウンロードした際に，オンラインストレージの中の特定のフォルダに整理した状態で入っているようにしたくて，いじくった際の自分用メモ．以下のサイトを参考にしています．
Zotero で論文管理してみた話
やったこと  ZoteroのStandaloneをダウンロードしてインストール Zotero Connecterをブラウザに追加 Zotfileのアドオンをダウンロードしてメニューバーからアドオンに追加 Google Driveなどのオンラインストレージに論文をいれたいフォルダを作る Stand alone 版の「環境設定&amp;raquo;ファイルとフォルダ」で基本フォルダに，先ほど作ったフォルダを設定 ツールから選べるZotFileの設定から，Source Folder for Attaching New Filesに先ほどのフォルダを設定，Location of Filesにも先ほどのフォルダを設定 PDFをコネクターで取り込んだ際に，一緒に取り込まれたpdfファイルを右クリックしてzotofileのメニューからRenameするとそのフォルダの中に取り込まれる  </description>
    </item>
    
    <item>
      <title>RからeStatのAPIを使ってみる（その2）</title>
      <link>https://onoshima.github.io/posts/restat_packages/</link>
      <pubDate>Sun, 08 Mar 2020 16:50:26 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/restat_packages/</guid>
      <description>はじめに Rからe-Statを使うお勉強の記録．@yutannihilation さん作成のestatapiというパッケージがあり，それを使ったらとても楽だった．使い方は以下のページに書いてある．
https://qiita.com/kazutan/items/9c0b2dd0f055fde45cda
不登校の児童生徒の数 今回は，文部科学省の「児童生徒の問題行動・不登校等生徒指導上の諸課題に関する調査」という調査を使ってみて，都道府県別1000人あたりの不登校児童の数をグラフにしてみた．日本地図については谷村先生作成のNipponMapというパッケージを使った．使い方は以下のサイトを参考にした．
https://oku.edu.mie-u.ac.jp/~okumura/stat/nippon.html
使ったコード library(pacman) p_load(estatapi, keyring,dplyr, stringr,NipponMap,RColorBrewer) key_set(&amp;#34;e-stat&amp;#34;) #　統計表の検索 00400304は児童生徒の問題行動・不登校等生徒指導上の諸課題に関する調査 statsList &amp;lt;- estat_getStatsList( appId = key_get(&amp;#34;e-stat&amp;#34;), searchWord = &amp;#34;&amp;#34;, statsCode = &amp;#34;00400304&amp;#34; ) str(statsList) lists &amp;lt;- statsList %&amp;gt;% filter(str_detect(TITLE, &amp;#34;不登校児童生徒数&amp;#34;)) id_vector &amp;lt;- pull(lists, &amp;#34;@id&amp;#34;) # メタ情報 meta_info &amp;lt;- estat_getMetaInfo(appId = key_get(&amp;#34;e-stat&amp;#34;), statsDataId = id_vector[1]) names(meta_info) # 実際のデータを取得 df &amp;lt;- estat_getStatsData( appId = key_get(&amp;#34;e-stat&amp;#34;), statsDataId = id_vector[1], cdCat02 = c(&amp;#34;120&amp;#34;, &amp;#34;140&amp;#34;), #小学校と中学校 cdArea = meta_info$area$&amp;#34;@code&amp;#34;[2:48]　#1番目は全国なので除外 ) df2017 &amp;lt;- df %&amp;gt;% rename(&amp;#34;year&amp;#34; = &amp;#34;時間軸(年度次)&amp;#34;) %&amp;gt;% filter(year == &amp;#34;2017年度&amp;#34;, 学校種類 == &amp;#34;小学校&amp;#34;, cat01_code == &amp;#34;120&amp;#34;) df2017_jhs &amp;lt;- df %&amp;gt;% rename(&amp;#34;year&amp;#34; = &amp;#34;時間軸(年度次)&amp;#34;) %&amp;gt;% filter(year == &amp;#34;2017年度&amp;#34;, 学校種類 == &amp;#34;中学校&amp;#34;, cat01_code == &amp;#34;120&amp;#34;) # 小学生をplot kenmei &amp;lt;- df2017$地域 br &amp;lt;- seq(3, 9, 1) color &amp;lt;- brewer.</description>
    </item>
    
    <item>
      <title>RからeStatのAPIを使ってみる</title>
      <link>https://onoshima.github.io/posts/rapi/</link>
      <pubDate>Sat, 07 Mar 2020 20:58:44 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/rapi/</guid>
      <description>Rからe-StatのAPIを使ったときのメモ．以下のサイトの言う通りにやってみただけ．
https://qiita.com/nozma/items/f88f5cc60ab63461deae
e-StatのアプリケーションIDの取得 使うためには，アプリケーションIDというものを取得しないとならない．
https://www.e-stat.go.jp/api/api-info/api-guide
上のサイトの指示通りに，ユーザー登録してマイページよりアプリケーションIDを発行する．
認証情報の管理 keyringというパッケージを使うと，ソースコードの中にIDとか書かなくてすむらしい．便利．
https://qiita.com/nozma/items/9f4a638906471d125e96
特別支援学校高等部の進路 試しに，文科省の「学校基本調査」から特別支援学校高等部で知的障害の進路の情報をとって，グラフにしてみたのがこちら．統計表をダウンロードしてからdplyrを使って整形していく流れ．実際の数と割合をグラフにしてみた．
使ったコードは以下のもの．なぜか平成23年度から25年度はひとまとまりで提供されていたり，平成22年度とそれ以降で列名が違ったりと下処理が面倒くさかったです．
library(pacman) p_load(keyring, httr, listviewer, rlist, pipeR, dplyr, stringr, ggplot2, stringi, readr, ggthemes) keyring::key_set(&amp;#34;e-stat&amp;#34;) ## 統計表を検索してお目当ての表を特定 response &amp;lt;- GET( url = &amp;#34;https://api.e-stat.go.jp/rest/2.1/app/json/getStatsList&amp;#34;, query = list( appId = keyring::key_get(&amp;#34;e-stat&amp;#34;), searchWord = &amp;#34;障害種 AND 状況別 AND 高等部&amp;#34; ) ) res_content &amp;lt;- content(response) jsonedit(res_content) id_table &amp;lt;- res_content$GET_STATS_LIST$DATALIST_INF$TABLE_INF %&amp;gt;&amp;gt;% list.select( id = `@id`, year = STATISTICS_NAME_SPEC$TABULATION_SUB_CATEGORY1, table_name = TITLE_SPEC$TABLE_NAME) %&amp;gt;&amp;gt;% list.stack() id_list &amp;lt;- pull(id_table, id) res &amp;lt;- tibble() for (id in id_list){ dat_year &amp;lt;- GET( url = &amp;#34;https://api.</description>
    </item>
    
    <item>
      <title>Mac miniに買い換えてやったこと</title>
      <link>https://onoshima.github.io/posts/macmini/</link>
      <pubDate>Sat, 22 Feb 2020 13:05:54 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/macmini/</guid>
      <description>家のパソコンを買い換えた．最初にやったことの備忘録．
 DOCKからいらないアプリを消す 入力関連の変更（日本語入力ソースで全角カンマと全角ピリオド・バックスラッシュなど） Mac OS Catalinaにアップデート Brew Bundleで Karabiner-Elementで日本語入力切替ショートカット追加 terminalのテーマをicebergに変更 zshのフレームワークにPreztoを入れる apm install &amp;ndash;packages-file apmlist.txt でまとめてatomのパッケージ入れる  </description>
    </item>
    
    <item>
      <title>信頼性係数の目安の出どころ</title>
      <link>https://onoshima.github.io/posts/nunnally/</link>
      <pubDate>Mon, 17 Feb 2020 20:43:28 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/nunnally/</guid>
      <description>尺度が開発される際には信頼性と妥当性についての検討が必要とされます。信頼性の指標としてはクロンバックのαが報告されることが多いのですが，巷でαの値が0.8以上あると望ましいとかなんとか言われていたりします。
例えば次のサイトではこんな風に書かれています。
 α係数の目安は0.80以上です。0.90を超えれば、かなりの信頼性と言えます。
 http://www.u-gakugei.ac.jp/~kishilab/validity-reliability.htm
また別のサイトではこんな風に書かれています。
 通常、アルファ係数が0.8以上であれば一貫性があると見なされます。
 https://bellcurve.jp/statistics/blog/12206.html
ところで，この0.8だの0.9だのの出どころについて調べてみると，英語で書かれたもののいくつかはNunnally(1978)というものを引用しています。これは次の本みたいです。
https://www.amazon.co.jp/Psychometric-Theory-McGraw-Hill-Psychology-Nunnally/dp/0070474656
では，そこには何が書いてあるのかと図書館でみてみますと以下のような記述でした。長いけど引用しておきます。
 What a satisfactory level of reliability is depends on how a measure is being used. In the early stages of research on predictor tests or hypotesized measures of a construct, one saves time and energy by working with instruments that have ony modest reliability, for which purpose reliabilities of .70 or higher will suffice. If significant correlations are found, corrections for attenuation will estimate how much the correlations will increase when reliabilities of measures are increased.</description>
    </item>
    
    <item>
      <title>Hugoで記事中に数式を書く際の_問題</title>
      <link>https://onoshima.github.io/posts/underscore/</link>
      <pubDate>Sat, 15 Feb 2020 16:32:13 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/underscore/</guid>
      <description>Hugoで記事中に数式を書いているとたまにうまくいかないことがある．例えば，次のような数式を表示したいとする．
これを，以下のように打つと
$$ X = X_{1A} + X_{2A} $$ こんなかんじで失敗する．
$$ X = X_{1A} + X_{2A} $$
これはMarkdownが先に _（アンダースコア）によって囲まれた部分をイタリックとして解釈してしまうことにより起きるらしい．これを避けるためには次のように _ をバックスラッシュでエスケープしてやるとよいらしい．
$$ X = X\_{1A} + X\_{2A} $$ そうすると次のようにうまいこと表示されているはず．
$$ X = X_{1A} + X_{2A} $$</description>
    </item>
    
    <item>
      <title>gitで一部のファイルを特定のコミットに戻すとき</title>
      <link>https://onoshima.github.io/posts/git_use/</link>
      <pubDate>Tue, 11 Feb 2020 17:30:47 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/git_use/</guid>
      <description>備忘録．なぜか知らないけれど，全然違うファイルを別のファイルに上書き保存してしまったときに調べた．
git log #戻したいコミット番号を得る git checkout [コミット番号] [ファイルパス] 参考
https://qiita.com/ritukiii/items/5bc8f74dbf4dc5d1384c</description>
    </item>
    
    <item>
      <title>lavaanのアウトプットをパス図にする</title>
      <link>https://onoshima.github.io/posts/lav2tikz/</link>
      <pubDate>Sat, 08 Feb 2020 11:57:05 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/lav2tikz/</guid>
      <description>はじめに RでSEMをやるときはlavaanをよく使うのですがその結果を綺麗なパス図にしたいということで関数を作りました. lavaanのアウトプットを突っ込むとTikZで描画するためのTeXファイルを吐き出す関数です. 構造方程式を含むもの（フルSEMとか呼ばれるのかな？）には対応しておらず, CFAをそれっぽく出すだけの機能しかありませんが&amp;hellip;
インターネットには有難い情報を提供している方がいるもので, 作る際にはbob3さんの以下のブログが参考になりました.
lavaanで作ったモデルからパス図を描く関数
使い方 みんな大好き Holzinger &amp;amp; Swinefordのデータで試してみます. lavaanのアウトプットを関数に突っ込みます. sizeの引数にはTeXの文字サイズの記号を入れると推定値の文字サイズが変えられます.
library(lavaan) mo &amp;lt;- &amp;#39; visual =~ x1+x2+x3 textual =~ x4+x5+x6 speed =~ x7+x8+x9 &amp;#39; res1 &amp;lt;- cfa(mo, HolzingerSwineford1939) write(lav2tikz(res1, size=&amp;#34;scriptsize&amp;#34;), file=&amp;#34;res1.tex&amp;#34;) 作業ディレクトリにres1.texファイルが作られますので, ターミナルでそこまで移動してコンパイルします.
$ latex res1.tex $ dvipdfmx res1.dvi 次のような結果が得られます（webにアップロード用にpngにしていますが）.
高次因子分析のモデルも一応描画できるようにしています.
mo1 &amp;lt;- &amp;#39; visual =~ x1+x2+x3 textual =~ x4+x5+x6 speed =~ x7+x8+x9 g =~ visual+textual+speed &amp;#39; res2 &amp;lt;- cfa(mo1, data=HolzingerSwineford1939) write(lav2tikz(res2, size=&amp;#34;scriptsize&amp;#34;), file=&amp;#34;res2.tex&amp;#34;) 結果は次の通り.</description>
    </item>
    
    <item>
      <title>TikZで遊んでみた</title>
      <link>https://onoshima.github.io/posts/tikz/</link>
      <pubDate>Sun, 02 Feb 2020 19:01:51 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/tikz/</guid>
      <description>きれいなパス図を描きたいと思ってTikZを試してみたときの記録．
TikZとは LaTeXで動くパッケージで図形をコマンドで描けるものらしい．以下のサイトが日本語で簡単な説明している．
https://guides.lib.kyushu-u.ac.jp/c.php?g=774891&amp;amp;p=5559083
もっと潜在変数を含めた心理学よりの説明をしているのだと以下のQ&amp;amp;Aとかを参考にした．
https://tex.stackexchange.com/questions/397610/latent-variable-model-with-tikz
因子分析のパス図を書いてみる とりあえず\nodeやら\drawやら\pathやらで必要なものを定義して，あとは[ ]に必要なオプションを入れて微調整する感じ．繰り返し処理とかを使えば，もっと短くなるらしいが，とりあえず最初なのでとりあえずコピペ多用でやってみた．lavaanのパッケージにも入っている有名な知能検査の因子分析のもの．
\documentclass[tikz,border=10pt]{standalone} \usetikzlibrary{shapes, arrows} \begin{document} \begin{tikzpicture} % node ovserved  \node (X1) at (0,0) [rectangle, draw]{$X_1$}; \node (X2) at (1,0) [rectangle, draw]{$X_2$}; \node (X3) at (2,0) [rectangle, draw]{$X_3$}; \node (X4) at (3,0) [rectangle, draw]{$X_4$}; \node (X5) at (4,0) [rectangle, draw]{$X_5$}; \node (X6) at (5,0) [rectangle, draw]{$X_6$}; \node (X7) at (6,0) [rectangle, draw]{$X_7$}; \node (X8) at (7,0) [rectangle, draw]{$X_8$}; \node (X9) at (8,0) [rectangle, draw]{$X_9$}; % node latene  \node (visual) at (1,1.</description>
    </item>
    
    <item>
      <title>続・2群のプリポストデータに関して</title>
      <link>https://onoshima.github.io/posts/prepost2/</link>
      <pubDate>Sat, 01 Feb 2020 20:09:15 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/prepost2/</guid>
      <description>以前に書いたもの 以前に2群のぷりポストデータについての記事を書いた. （2群のプリポストデータに関して）
それに, 関連した文献を読んだの記録を残しておく.
プリポストデータへの分析の使い分け 小杉先生が翻訳したグリム・ヤーノルド『研究論文を読み解くための多変量解析入門応用編』（北大路書房）に書いてある. pp.279-282が該当するページ.
行動科学の研究者は以下の3つの方法を用いる傾向があるそうだ.
 時間を群内変数とし, グループを群間変数とする反復測定のANOVA プレテストを共変量とするANCOVA 差得点に対するANOVA  これらのうちで3つ目を選ぶ人は少ないようである.
最初のANOVAについては「最も倹約的でない方法」とされている. 研究者が興味があるところは, グループ×時間の交互作用で表されており, そこの有意性が確認されたのちに, 単純主効果の検定および多重比較が必要になるので, 複数のステップが必要になる. 交互作用のF値は, 差得点の一元配置のANOVAのF値と等しいので, 後者の方が効率的であるとされている.
差得点のANOVAとANCOVAの使い分けについては研究デザインによるとされている.
具体的には, 実験参加者のグループへの割り当てが完全にランダマインズされたデザインでは, ANCOVAはより強力なツールになるようである
擬似実験だとやや事情が異なり, プレテストのスコアがグループ間で異なる場合には, 差得点の分析を行うのが妥当であり, グループ間でスコアが異ならない場合にはとANCOVAが強力だと書いてある.
上記の説明 詳しくは, Huck &amp;amp; McLean, 1975とMaxwell &amp;amp; Delaney, 1990に書いてあるらしい. （読んでいないけど）
 https://psycnet.apa.org/doiLanding?doi=10.1037%2Fh0076767 https://psycnet.apa.org/record/1990-97136-000  </description>
    </item>
    
    <item>
      <title>Zettlr関係の備忘録</title>
      <link>https://onoshima.github.io/posts/zettlr/</link>
      <pubDate>Sat, 01 Feb 2020 18:17:51 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/zettlr/</guid>
      <description>私が家で使っているパソコンWordが強烈に重くてタイプしてから画面に表示されるまでにえらく時間がかかるので．自分用のいろいろなものはMarkdownで打っています．それでMarkdownエディタを色々と試して見ているのですが．その中で見つけたZettlrというものがよさそうなのでその関連のメモ．
Zettlrとは 論文執筆のために設計されたMarkdownエディタのようです．開発しているのはドイツの社会学者の方だそうです．オープンソースでマルチプラットフォームです． https://tomochemist.com/2019/02/21/post-279/
インストール関係 公式のダウンロードページからいけます．
https://www.zettlr.com/download
Macなら brew caskでいけました．
$ brew tap caskroom/cask $ brew cask install Zettlr Ubuntu18.04ではインストーラをダウンロードしてインストールすれば問題なく動いています．
設定関連 日本語を含むファイルをpdf出力するのに少し苦戦しました．
https://qiita.com/sky_y/items/15bf7737f4b37da50372
上記のサイトを参考に，BXjsclsクラスを使う戦略をとり「設定＞高度な設定＞pandocコマンド」の部分を変更してみます．デフォルトだと以下のものが入っています.
pandoc &amp;quot;$infile$&amp;quot; -f markdown $outflag$ $tpl$ $toc$ $tocdepth$ $citeproc$ $standalone$ --pdf-engine=xelatex -o &amp;quot;$outfile$&amp;quot; それを以下のように変えると, とりあえず日本語を含むものでも出力できるようになりました.
pandoc &amp;quot;$infile$&amp;quot; -f markdown $outflag$ $toc$ $tocdepth$ $citeproc$ -o &amp;quot;$outfile$&amp;quot; $standalone$ --pdf-engine=xelatex -V documentclass=bxjsarticle -V classoption=pandoc Ubuntu18.04だとこれで問題なかったのですが, MacだとIPAフォントが入っていないと怒られましたbrew cask installfont-ipaexfontしてもそんなファイルないよと断られたので, (20200218追記) brew install homebrew/cask-fonts/font-ipaexfont でいけました。homebrewの使い方を把握していないだけでした。
以下から直接ダウンロードしてフォントブックに入れると無事に日本語ファイルが出力できるようになりました．消し去った$tpl$がなんなのかはよく分かりませんが&amp;hellip;
https://ja.osdn.net/projects/ipafonts/releases/
引用機能（2020/2/9追記） Documentationにしたがって引用機能を試してみました．手順は以下の通り.
 BetterBibTexプラグインをZoteroに入れる Zoteroから文献全てをエクスポートする（その際フォーマットはBetter CSL JSONを選択） Zettlrの設定&amp;gt;エクスポートのタブから今出力した文献のjsonファイルを選択する  これで準備OKです．</description>
    </item>
    
    <item>
      <title>RstudioでGithubの鍵認証設定</title>
      <link>https://onoshima.github.io/posts/rstudiogithub/</link>
      <pubDate>Sat, 25 Jan 2020 14:04:09 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/rstudiogithub/</guid>
      <description>R studioの鍵認証についての備忘録．またやることになったらおそらく忘れるので．
やったこと  RstudioのToos &amp;gt; Global Option &amp;gt; Git/SVNでView public keyをクリック 表示される公開鍵をコピー Githubの自分のページ &amp;gt; Settings &amp;gt; SSH and GPG keysから右上のNew SSH kayというボタンをクリック 適当な名前をつけて先ほどの公開鍵をペースト  参考にしたもの http://dragstar.hatenablog.com/entry/2016/04/26/185546</description>
    </item>
    
    <item>
      <title>ggplot2でpdfを出力する</title>
      <link>https://onoshima.github.io/posts/ggplot2japanese/</link>
      <pubDate>Sun, 12 Jan 2020 23:12:22 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/ggplot2japanese/</guid>
      <description>ggplot2でpdf出力 # ggplot2 pdf出力 library(tidyverse) dat &amp;lt;- as_tibble(iris) dat_long &amp;lt;- dat %&amp;gt;% gather(key=&amp;#34;part&amp;#34;, value=&amp;#34;value&amp;#34;,-Species) %&amp;gt;% mutate(part = as.factor(part)) p &amp;lt;- ggplot(dat_long, aes(x=Species, y=value, fill=Species)) + geom_boxplot() + facet_wrap(~part) # pdfに出力 pdf(&amp;#34;figure/iris.pdf&amp;#34;, width=7, height=7) print(p) dev.off() ※ 画像web表示のためにpng形式.
日本語ファイルを出力 pdfで使うフォントを指定してやるとよい. pdfで何のフォントが使えるかを知りたい時はnames(pdfFonts())とか打つと良い.
# 日本語を使う p1 &amp;lt;- p + ggtitle(&amp;#34;日本語タイトル&amp;#34;) + theme(plot.title=element_text(family=&amp;#34;Japan1GothicBBB&amp;#34;)) pdf(&amp;#34;figure/iris_ja.pdf&amp;#34;, width=7, height=7) print(p1) dev.off() ※ 画像web表示のためにpng形式.
フォントが入っていない環境でも同じように表示するために, 以下のようにしてフォントを埋め込んでおくと良いらしい. embedFonts(&amp;#34;figure/iris.pdf&amp;#34;)</description>
    </item>
    
    <item>
      <title>相関係数行列からクロンバックのαを求める</title>
      <link>https://onoshima.github.io/posts/cronbach/</link>
      <pubDate>Fri, 27 Dec 2019 21:37:22 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/cronbach/</guid>
      <description>尺度の作成論文などでは信頼性係数が報告される。信頼性係数には様々な種類があるが, その中でもっともよく使われるのがクロンバックのαである。クロンバックのαは次の計算式で求められる。
$$ a = \frac{n}{n-1} \left( 1- \frac{\Sigma^n _{i=1}Var(X_i)}{Var(X)}\right) \tag{1} $$
ここで, $n$は項目数, $X_i$は$i$番目の項目の得点である。$X$はテストの合計得点である。
Rでクロンバックのαの求めるには Rでクロンバックのαを求めるためには複数のやり方がある。
http://www.okadajp.org/RWiki/?%CE%B1%E4%BF%82%E6%95%B0%EF%BC%8C%E4%BF%A1%E9%A0%BC%E6%80%A7%E4%BF%82%E6%95%B0%E3%81%AE%E7%AE%97%E5%87%BA
とりあえずここではpsychパッケージに含まれるalpha()関数を使ってみることにする。データは同じパッケージに含まれるbfiを用いる。このデータはビッグファイブ理論が想定する因子を測定する質問紙で1因子につき5項目合計25項目からなる。サンプルサイズは2800である。今回は最初の10項目だけ使う。
データの中身はこんな感じ。6件法である。
さて, このデータに対してクロンバックのαを求めるにはalpha()関数を用いるのだが, データが逆転項目を含んでいるのでそれを処理しなければいけない。6件法なので6から項目を引くと出てくる。ここではのちの話の都合で欠損を含むデータを除外している。
# 逆転項目の処理して必要な項目だけ取り出す new &amp;lt;- dat %&amp;gt;% mutate(A1=6-A1, C4=6-C4, C5=6-C5) %&amp;gt;% select(1:10) %&amp;gt;% filter(!is.na(A1), !is.na(A2), !is.na(A3), !is.na(A4), !is.na(A5), !is.na(C1), !is.na(C2), !is.na(C3), !is.na(C4), !is.na(C5)) 逆転項目の処理が済んだところで, alpha()関数を使ってみる。
# クロンバックαを計算 res &amp;lt;- psych::alpha(new) αの値が出てくるだけでなくて, 各項目をなくした際にαがどのように変化するかの値も出てくる。これらは尺度作成の際には役に立つだろう。
相関係数からクロンバックのαを求める ところで岡田（2015）によれば, クロンバックのαは相関係数行列から求めることもできるようである。
https://www.jstage.jst.go.jp/article/arepj/54/0/54_71/_article/-char/ja/
相関係数行列からクロンバックのαは次の式で求められる。
$$ a = \frac{n\bar{r}}{1+(n-1) \bar{r}} \tag{2} $$
ここでnは項目数で, $\bar{r}$は項目間の相関係数の平均である。さきほどのデータを用いて, 同様の結果が得られるが試してみる。相関係数行列を求めて, 上記の式に当てはめてみる。</description>
    </item>
    
    <item>
      <title>2群のプリポストデータに関して</title>
      <link>https://onoshima.github.io/posts/prepost/</link>
      <pubDate>Sat, 21 Dec 2019 16:12:33 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/prepost/</guid>
      <description>2群のプリポストデータ 理由はよくわからないのだけれど, 知り合いから2群のプリポストデータの分析について聞かれることが多い。何かしらの介入の効果を検討したいらしく、統制群と介入群の2つに分けて, それぞれの群で介入前と介入後のデータを取るみたいな状況を想定しているようだ。
こうした場合に適切な分析手法については吉田（2006）が論じており, （1）ポスト-プリの差得点を取り, その差得点（変化量）を従属変数としてt検定を行う, （2）プリ得点を共変量とした共分散分析（ANCOVA）を行う, の2種類が推奨されている。
2要因（群×時期）の分散分析を行う例が散見されるが, これはこの分析において見たいものが交互作用のみで主効果の分析が意味をなさないので冗長だとされている。なお, この2要因分散分析の結果得られた交互作用のF値は, (1)の手法で得られた検定統計量tの2乗に一致することが知られているそうである。
実際のデータを使った例 実際のデータといっても『やさしく学べる統計学』にあるサンプルデータを使う。データはcsv形式で, 青山学院の寺尾先生のホームページのシラバスからダウンロードできる。
http://www.cc.aoyama.ac.jp/~t41338/lecture/aoyama/stat2e/stat2e_top.html
データの中身の一部を取りだすとこんな感じである。1列目に被験者番号, 2列目に群の情報, 3列目にプリ得点, 4列目にポスト得点。なんの介入なのかは知らないけれどこんな形でデータが得られたとする。統制群と介入群は8名ずつである。
箱ひげ図で表してみると次の通り。
# データセットをlong型にして必要な列をfactorに変換 dat.long&amp;lt;- dat %&amp;gt;% gather(key = time, value = value, -c(group,sub)) %&amp;gt;% mutate(sub = as.factor(sub),group = as.factor(group), time= factor(time, levels=c(&amp;#34;pre&amp;#34;,&amp;#34;post&amp;#34;))) ## データの可視化 ggplot(data=dat.long, aes(x=time,y=value)) + geom_boxplot() + facet_wrap(~group) 差の変数のt検定 まず, （1）の方法を試してみる。 「post-pre」の変数を作ってそれからt検定を行う。
# 差の得点を作ってt検定 newdat &amp;lt;- dat %&amp;gt;% mutate(difference = post-pre) %&amp;gt;% mutate(group = as.factor(group)) res.t &amp;lt;- t.</description>
    </item>
    
    <item>
      <title>Rでベクタ画像を作る際のメモ</title>
      <link>https://onoshima.github.io/posts/pdf/</link>
      <pubDate>Tue, 30 Jul 2019 18:20:19 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/pdf/</guid>
      <description>Rでベクタ画像を作成する際に色々調べたメモ。
ベクタ画像の作り方 postscriptでデバイスを開いて次のようにすると良い。
postscript(&amp;#34;eps_sample1.eps&amp;#34;, width=7, height=7, paper=&amp;#34;special&amp;#34;, onefile = F, horizontal = F) plot(1:10, main=&amp;#34;eps file&amp;#34;) dev.off() widthとheightはインチ指定で描画する領域。paperはその外側の領域で&amp;quot;a4&amp;quot;とかサイズを指定するそうだけれど純粋に描いた領域の画像が欲しい場合は&amp;quot;special&amp;quot;を指定しておくと良いらしい。
onefileは1つのファイルに2つ以上の画像を埋め込むとかどうたらの設定らしいが, 指定しないで作ったら私の環境だとよく分からない余白が発生した。Rwikiによれば, horizontalを指定しておかないとLaTeXに取り込んだ際によくないことが起こるらしい。
フォントについて フォントを変更したいときは,引数でfamily=&amp;quot;Times&amp;quot;のように指定してやると良い。
postscriptでどんなフォントが使えるかが知りたければ, &amp;lsquo;postscriptFonts()&amp;lsquo;の引数で見ることができる。Japan1というのがあるのでそれを選べば日本語が出力できる。（私のUbuntuの環境だとできるがMacだとなぜかできない。）
postscript(&amp;#34;eps_sample4.eps&amp;#34;, width=7, height=7, paper=&amp;#34;special&amp;#34;, onefile = F, horizontal = F, family=&amp;#34;Japan1&amp;#34;) plot(1:10, main=&amp;#34;日本語のフォント&amp;#34;) dev.off() フォントの埋め込み こうして作られたepsファイルはフォントが埋め込まれていないため, 当該のフォントが入っていない環境では表示されなくなってしまう。そこで出来上がった画像にフォントを埋め込む必要がある。といっても、関数一発でできるので難しいことはない。
embedFonts(&amp;#34;eps_sample1.eps&amp;#34;, format = &amp;#34;eps2write&amp;#34;) 参考にしたサイト Rのポストスクリプト画像（eps・psファイル）出力
R による EPS 画像の作成とその LATEX への取り込み</description>
    </item>
    
    <item>
      <title>Rのlapply関数に関するメモ</title>
      <link>https://onoshima.github.io/posts/laaply/</link>
      <pubDate>Fri, 26 Jul 2019 13:47:56 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/laaply/</guid>
      <description>lapplyおよびparallelパッケージのmclapplyについて調べた際の記録。
lapplyとは Rで同一の関数を複数のオブジェクトを対象に行うときには, forで繰り返しのループで書くよりも, apply()ファミリーを用いて並列的に処理したほうが早いと言われる。lapplyは, 与えられたリストに対して同一の関数を適用する。lapplyだと戻り値はリストで, sapplyだと戻り値はベクトルである。
例：異なる長さのベクトルの平均 実際に例を見てみると理解が早いかもしれない。長さが10, 100, 1000のベクトルからなるリストそれぞれの平均を求めるには次のように入力する。
x &amp;lt;- list(rnorm(10), rnorm(100), rnorm(1000)) lapply(x, mean) # 戻り値はリスト sapply(x, mean) # 戻り値はベクトル mclapplyとは lapplyではリストのそれぞれに並列の処理を行うことができることが分かったが, 複数のリストオブジェクトに対して並列的に処理を行いたいという場面もあるかもしれない。そうした際には&amp;rsquo;parallel&amp;rsquo;というパッケージの&amp;rsquo;mclapply&amp;rsquo;という関数が使えるらしい。
使い方は次のとおり。で繰り返す回数と, 処理する関数を指定するとよいようだ。
replist &amp;lt;- parallel::mclapply(1:1000, function(x){ replicates &amp;lt;- list(mean(rnorm(10)), mean(rnorm(100)),mean(rnorm(1000))) }) 戻り値はネストされたリストである。例えば, ネストされたリストを次のように行列の形に直せば, 異なるサンプルサイズにおける標本平均値のばらつきなども分かる。
replicates &amp;lt;- matrix(unlist(replist), 1000, 3, byrow=T) boxplot(replicates, names=c(&amp;#34;size10&amp;#34;, &amp;#34;size100&amp;#34;, &amp;#34;size1000&amp;#34;)) 速度の比較 本当にforのループよりも早いか調べてみる。今の処理を100,000回繰り返してみて速度を測ることにする。
t.loop &amp;lt;- proc.time() x &amp;lt;- list() for (i in 1:100000){ x[[i]] &amp;lt;- list(mean(rnorm(10)), mean(rnorm(100)), mean(rnorm(1000))) } t.loop &amp;lt;- proc.</description>
    </item>
    
    <item>
      <title>ブートストラップについての調べもの</title>
      <link>https://onoshima.github.io/posts/boot/</link>
      <pubDate>Wed, 03 Jul 2019 07:10:40 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/boot/</guid>
      <description>Rでブートストラップについて調べたときの記録。
ブートストラップとは 大雑把に言えば, 統計量とかの分布を調べたりするために1つの標本から新しい標本（ブートストラップ標本）を何個も作ることのようだ。パラメトリックとノンパラメトリックがある。
今回扱うのはノンパラメトリックの方。
自作関数によるブートストラップ ブートストラップ標本を作るときには, もとの標本から同じサイズで復元抽出を行う。復元抽出とは要するに, くじ引きでくじを引くたびにもとの箱にくじを戻すことである。たとえば, 【太郎さん、花子さん、和夫さん】の3人からなるもとの標本があった場合に, あるブートストラップ標本では【太郎さん, 太郎さん, 花子さん】になるかもしれないし, 別のブートストラップ標本では【和夫さん, 和夫さん, 和夫さん】となるかもしれない。Rで復元抽出を行う際にはsamaple()関数の引数でreplace=TRUEを指定すると良い。試しにやってみる。Rの組み込みのデータセットであるstateからアメリカ50州の収入のデータを使って, 平均値と中央値がどのように分布するかを見てみる。
# もとの標本 original &amp;lt;- state.x77[,&amp;#34;Income&amp;#34;] # 元の標本から復元抽出する回数 num.bs.sample &amp;lt;- 1000 # 結果を保存するようベクトル median.vector &amp;lt;- numeric(num.bs.sample) mean.vector &amp;lt;- numeric(num.bs.sample) for (i in 1:num.bs.sample){ median.vector[i] &amp;lt;- median(sample(original, length(original), replace=TRUE)) mean.vector[i] &amp;lt;- mean(sample(original, length(original), replace=T)) } # 結果の図示 boxplot(median.vector, mean.vector, names=c(&amp;#34;Median&amp;#34;,&amp;#34;Mean&amp;#34;), horizontal = T, las=T, xlab=&amp;#34;Income&amp;#34;) 結果は次のとおりである。これを使うと中央値の経験的な分布なども求められる。
bootパッケージによるブートストラップ 毎回, 自分で繰り返しのループを書くのも面倒であろうということでbootという便利なパッケージがある。boot(もとの標本, 統計量を求める関数, ブートストラップ標本数)のように使う使うらしい。同じデータでやってみよう。
library(boot) foo &amp;lt;- function(d,k){ median.</description>
    </item>
    
    <item>
      <title>tidyrを使ったデータの整理</title>
      <link>https://onoshima.github.io/posts/filter/</link>
      <pubDate>Tue, 25 Jun 2019 16:38:21 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/filter/</guid>
      <description>データ整理にtidyverseを使った際の覚書。まずデータフレームをas_tibble()でtidy用の形式にする。
library(tidyverse) air &amp;lt;- airquality tidy.air &amp;lt;- as_tibble(air) 条件に合うケースの抽出 filter()関数を使うと良いらしい。
## 条件に合う行を抽出 air.May &amp;lt;- tidy.air %&amp;gt;% filter(Month==5) #5月のデータのみ取り出す Ozone.mean &amp;lt;- mean(air$Ozone, na.rm=T) air.high.Ozone &amp;lt;- tidy.air %&amp;gt;% # オゾンが平均より高いケースを取り出す filter(Ozone &amp;gt; Ozone.mean) air.high.Ozone.May &amp;lt;- tidy.air %&amp;gt;% #複合条件もOK filter(Ozone &amp;gt; Ozone.mean, Month==5) 抽出されたケースを見ると, オゾンの値が平均（43くらい）よりも高いケースのみのデータセットが出来上がっている。</description>
    </item>
    
    <item>
      <title>テトラコリック相関係数を求める関数の自作</title>
      <link>https://onoshima.github.io/posts/190619tetra/</link>
      <pubDate>Wed, 19 Jun 2019 21:59:21 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/190619tetra/</guid>
      <description>先日テトラコリック相関係数を求める方法についての記事を書いた。
テトラコリック相関の初期
それに基づいてRでテトラコリック相関係数を求める関数を自作した。
Rでの関数化 作った関数は以下の通り。引数には2×2の相関表を指定する。途中のテトラコリック関数（？）で何次まで計算するかということをorder=という引数で指定する。デフォルトは論文に掲載されていたものと同じ6にしてある。
tetrachoric &amp;lt;- function(cor.table, order=6){ #必要な数値を代入 N &amp;lt;- sum(cor.table) prop.bd &amp;lt;- colSums(prop.table(cor.table))[2] prop.cd &amp;lt;- rowSums(prop.table(cor.table))[2] h &amp;lt;- qnorm(prop.bd, lower.tail = F) H &amp;lt;- dnorm(h) k &amp;lt;- qnorm(prop.cd, lower.tail = F) K &amp;lt;- dnorm(k) tau &amp;lt;- numeric(order) theta &amp;lt;- numeric(order) v_bar &amp;lt;- numeric(order) w_bar &amp;lt;- numeric(order) #テトラコリック関数 for(n in 1:order){ if(n==1){ v_bar[n] &amp;lt;- h w_bar[n] &amp;lt;- k tau[n] &amp;lt;- H theta[n] &amp;lt;- K } else if (n==2){ v_bar[n] &amp;lt;- h^2-1 w_bar[n] &amp;lt;- k^2-1 tau[n] &amp;lt;- H*h/sqrt(2) theta[n] &amp;lt;- K*k/sqrt(2) } else { v_bar[n] &amp;lt;- h * v_bar[n-1] - (n-1) * v_bar[n-2] w_bar[n] &amp;lt;- k * w_bar[n-1] - (n-1) * w_bar[n-2] tau[n] &amp;lt;- H*v_bar[n-1]/sqrt(factorial(n)) theta[n] &amp;lt;- K*w_bar[n-1]/sqrt(factorial(n)) } } # 相関を計算 coef_vec &amp;lt;- c(prop.</description>
    </item>
    
    <item>
      <title>テトラコリック相関の初期</title>
      <link>https://onoshima.github.io/posts/190618everitt/</link>
      <pubDate>Tue, 18 Jun 2019 15:10:22 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/190618everitt/</guid>
      <description>テトラコリック相関係数関連の論文を読んだメモ。テトラコリック相関を計算するときに便利な表を作ったぞという話。
書誌情報  Everitt, P. F. (1910). Tables of the tetrachoric functions for fourfold correlation talbes. Biometrika, 7(4), 437–451. https://doi.org/10.1093/biomet/7.4.437
 テーブルの説明 Pearson(1900)が発表したテトラコリック相関では以下のようなクロス表を想定して相関係数を求める。 (p.437)
左の図の実線はx軸とy軸である。点線の表す平面が二変量正規分布を切って4つの区画a,b,c,dに分けられてると考える。周辺確率を用いてhとkの値を求めることができる。
さて，この表から相関係数が求めるには以下の式が使われる。
$$ \frac{d}{N} = \frac{b+d}{N} \cdot \frac{c+d}{N}+S _ {1}^{\infty} (\frac{r^n}{n!}HK\bar{v} _ {n-1}\bar{w} _ {n-1}) \tag{1}$$
ここでHとKは正規曲線の縦座標（確率密度）である（Sは今の$\Sigma$）。$\bar{v}と\bar{w}$ は以下の式で与えられる。
$$ \bar{v} = h\bar{v} _ {n-1} - (n-1)\bar{v} _ {n-2}, \bar{v} _ 0 = 1, \bar{v} _ 1 = h $$ $$ \bar{w} = k\bar{w} _ {n-1} - (n-1)\bar{w} _ {n-2}, \bar{w}_0 = 1, \bar{w} _ 1 = k $$ これらを用いて，</description>
    </item>
    
    <item>
      <title>Ubuntu 18.04にCiscoAnyconnectを入れたときのメモ</title>
      <link>https://onoshima.github.io/posts/190618cisco/</link>
      <pubDate>Tue, 18 Jun 2019 10:48:11 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/190618cisco/</guid>
      <description>大学のVPN接続にCiscoのソフトウェアを入れる必要があったのだが，大学のマニュアルにはWindows用とMac用のインストール手順しか乗っておらず，ubuntuへの導入については色々と調べる必要があった．結果的に次のサイトで見つけたコードをターミナルに打ち込んだら問題なく導入された．
sudo apt-get update sudo apt-get install network-manager-openconnect sudo apt install libpangox-1.0-0 sudo apt-get install lib32z1 lib32ncurses5 cd ~ &amp;amp;&amp;amp; mkdir download-anyconnect cd download-anyconnect &amp;amp;&amp;amp; wget http://www.isthe.com/for-you-only/anyconnect/OLD/anyconnect-predeploy-linux-64-4.3.05017-k9.tar.gz tar -xzvf anyconnect-predeploy-linux-64-4.3.05017-k9.tar.gz cd /anyconnect-4.3.05017/vpn/ sudo ./vpn_install.sh sudo systemctl daemon-reload 参照：How to install Cisco Anyconnect on Ubuntu 18.04 64-Bit.</description>
    </item>
    
    <item>
      <title>Test</title>
      <link>https://onoshima.github.io/posts/test/</link>
      <pubDate>Thu, 13 Jun 2019 20:37:10 +0900</pubDate>
      
      <guid>https://onoshima.github.io/posts/test/</guid>
      <description>これはテスト投稿です。
ソースコードのハイライト表示 # パッケージを読み込み library(psych) # 記述統計を表示 describe(dat) # 繰り返し for (ite in 1:12){ x[ite] &amp;lt;- rnorm(1) } 数式の表示 $$ f(x) = e^{x}$$
文章中に$\theta_{i}$こんな感じでインライン表示も可能。</description>
    </item>
    
    <item>
      <title>Profile</title>
      <link>https://onoshima.github.io/profile/</link>
      <pubDate>Fri, 11 Jan 2019 18:58:53 +0900</pubDate>
      
      <guid>https://onoshima.github.io/profile/</guid>
      <description>略歴 早稲田大学大学院教育学研究科博士後期課程教育基礎学専攻に在籍。日本学術振興会特別研究員（DC2）。千葉県船橋市で生まれ神奈川県横浜市で育つ。早稲田大学大学院教育学研究科修士課程学校教育専攻修了。修士（教育学）。公認心理師。特別支援教育士（S.E.N.S）。神奈川県の私立中学校，特別支援学校教諭，板橋区教育委員会事務局指導室特別支援教育巡回指導講師，品川区立教育総合センター教育心理相談員，昭和大学発達障害医療研究所研究補助員を経て，現在に至る。専門は計量心理学。
研究分野 心理学におけるデータの測定法や分析法全般に関心があります。以前は，自閉スペクトラム症児者の支援（特にTEACCHプログラムの構造化および応用行動分析に基づくもの）についての研究をしていた関係で発達障害児者の支援も細々とやっています。
今までに以下のようなテーマに取り組んできました。
 順序カテゴリデータの分析法 テスト・尺度の信頼性 心理検査の妥当性 自閉スペクトラム症児者の支援  職歴  学校法人国際学園 星槎中学校 教諭：2011年4月-2015年3月 川崎市立田島支援学校桜校 臨時的任用教員：2015年4月-2017年3月 板橋区教育委員会事務局指導室 特別支援教育巡回指導講師：2017年11月-2019年3月 昭和大学発達障害医療研究所：研究補助員：2019年2月-現在 品川区立教育総合支援センター 教育心理相談員：2019年4月-2020年3月 鶴見大学短期大学部 非常勤講師：2020年4月-現在 NPO法人パルレ スタッフ：2020年4月-2021年3月 独立行政法人日本学術振興会 特別研究員（DC2）: 2021年4月-現在  学歴  早稲田大学教育学部英語英文学科：2007年4月-2011年3月 学士（文学） 早稲田大学大学院教育学研究科修士課程学校教育専攻：2017年4月-2019年3月 修士（教育学） 早稲田大学大学院教育学研究科博士後期課程教育基礎学専攻：2019年4月-現在  資格  2011年3月 中学校教諭1種免許状（外国語） 2011年3月 高等学校教諭１種免許状（外国語） 2015年3月 特別支援学校教諭2種免許状（知的障害・肢体不自由領域） 2018年10月 小学校教諭2種免許状 2018年12月 ADOS-2 臨床用ワークショップ　修了 2019年3月 中学校教諭専修免許状（外国語） 2019年3月 高等学校教諭専修免許状（外国語） 2019年4月 特別支援教育士（S.E.N.S.） 2019年8月 ADOS-2 研究用ワークショップ 修了 2019年10月 公認心理師  所属学会  日本LD学会：2015年4月-現在 日本自閉症スペクトラム学会：2018年4月-現在 日本心理学会：2018年4月-現在 日本教育心理学会：2020年4月-現在 日本行動計量学会：2021年4月-現在  連絡先 </description>
    </item>
    
  </channel>
</rss>
